{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c35940-0471-47ca-b1ba-690c0d8f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse as url\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f8231-996b-44be-a0a5-3edf522c85bb",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212a1c-6acd-4bef-9252-ee1c9b906a96",
   "metadata": {},
   "source": [
    "This notebook aims to explain the preprocessing steps to get the datasets that will be used for the analysis, respectively \"Actor.pkl\" and \"Movie.pkl\" <br> All these steps are indicative, you don't have to run all these cells, as running all the notebook is quite time-consuming (around 7 hours due to the part \"Data from Web\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8ec3d-fe0a-4b91-90a5-c6b09992892f",
   "metadata": {},
   "source": [
    "## IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec45fe5-7679-4471-9e06-2a1f5767eac1",
   "metadata": {},
   "source": [
    "First, you can go to https://datasets.imdbws.com, download the files and extract them :\n",
    "- title.basics.tsv.gz\n",
    "- title.crew.tsv.gz\n",
    "- title.ratings.tsv.gz\n",
    "- name.basics.tsv.gz\n",
    "  \n",
    "<br> Note that a descriptive of this dataset can be found on https://developer.imdb.com/non-commercial-datasets/ or in the README in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029194e-97d3-4be3-9471-c9f0c062ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path of the folder you have just downloaded \n",
    "path = \"/Users/alexandre/Desktop/Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5f375-f4ea-452b-85eb-8f7fd5b84a9a",
   "metadata": {},
   "source": [
    "Custom column names are defined :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1163a18-8537-46d3-9031-6e4cf1f1f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_CREW = ['IMDb_title_ID','IMDb_director_ID', 'IMDb_writers_ID']\n",
    "NAMES_ratings = ['IMDb_title_ID', 'Average rating', 'number of votes'] \n",
    "NAMES_BASICS = ['IMDb_people_ID', 'Name', 'birthYear', 'deathYear', 'profession', 'knownForTitles']\n",
    "NAMES_TITLES = ['IMDb_title_ID', 'TitleType', 'Primary_title', 'Original_title', 'isAdult', 'release_date', 'end_year', 'runtime', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a24d5-2398-4cbc-8134-88f5a51697b9",
   "metadata": {},
   "source": [
    "All datasets are loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b82ede-362e-4e45-beeb-10b0790bea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_crew = pd.read_csv(path+\"title.crew.tsv\", sep='\\t', names = NAMES_CREW, header = 0)\n",
    "df_IMDb_ratings = pd.read_csv(path+\"title.ratings.tsv\", sep='\\t', names = NAMES_ratings, header = 0)\n",
    "df_IMDb_title = pd.read_csv(path+\"title.basics.tsv\", sep='\\t', names = NAMES_TITLES, header = 0, low_memory=False)\n",
    "df_IMDb_name = pd.read_csv(path+\"name.basics.tsv\", sep='\\t', names = NAMES_BASICS, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960f395-8efd-427f-9f37-22c696a23df1",
   "metadata": {},
   "source": [
    "For the title.basics dataset, only movie must be selected from TitleType (drop series, TV episode, ...). Without this set, df_IMDb_title is huge.\n",
    "<br> A Few unnecessary columns for our project are also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dee27ca-0979-469d-986d-3957453816cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_type = 'movie'\n",
    "df_IMDb_title = df_IMDb_title.query('TitleType==@selected_type')\n",
    "df_IMDb_title=df_IMDb_title[['IMDb_title_ID', 'release_date','runtime','Primary_title', 'Original_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984dfdb-6bf8-40ea-827d-007fefeb8c2f",
   "metadata": {},
   "source": [
    "### Merging Titles, crews and ratings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c0683e-902d-4249-8799-30dea5a7325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697175, 5)\n",
      "(1498615, 3)\n",
      "(10571536, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_IMDb_title.shape)\n",
    "print(df_IMDb_ratings.shape)\n",
    "print(df_IMDb_crew.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c24a49-a482-44ff-b0c2-c3d7719ac922",
   "metadata": {},
   "source": [
    "The ratings and crew dataset contains also ratings of non-movie types. We merge on df_IMDb_title dataset as we have already selected the movie's row. (here with type of pd.merge to be performed: how = left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bc0958-77a5-4ca4-9fc9-ddaffc003508",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_title_rating = pd.merge(df_IMDb_title, df_IMDb_ratings, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de7186c-3c86-45e0-b8aa-7a0c2800761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb = pd.merge(IMDb_title_rating, df_IMDb_crew, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c777517-3859-43a9-9819-c78b1d39de76",
   "metadata": {},
   "source": [
    "IMDb dataset contains sometimes the value \"/N\" when information is unknown. Let's replace them with an NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2425fa6-95f0-4cbd-af37-df8536f92933",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb.replace(r'\\N', pd.NA, inplace=True)\n",
    "df_IMDb_name.replace(r'\\N', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f57636-e184-408b-81d6-2ba3d1cc39ae",
   "metadata": {},
   "source": [
    "### Match ID of writer/director with their name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd778f47-2cce-4776-aefc-877dcd07b46b",
   "metadata": {},
   "source": [
    "IMDb dataset contains only the ID of the writer and producer but not their name. In this section, we add 2 columns with the name of each one (if known)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d7669-1cea-46dd-a162-c0f4006be183",
   "metadata": {},
   "source": [
    "Defining a dict to be able to find the match easily :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f740f859-593b-4bce-b9f4-667eacf5d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_name.index = df_IMDb_name[\"IMDb_people_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacd328e-188c-4aff-adee-4d49d4df3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name = df_IMDb_name[[\"Name\",\"birthYear\",\"deathYear\"]].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae7aad-4323-49d8-985d-e2cebb813b66",
   "metadata": {},
   "source": [
    "Add a column for the name, birth year and death year (for each writer and producer). If there are multiple writers/producers, the infos are separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0076f3c3-8a19-4199-832e-73cd8061cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['Producer name'] = IMDb['IMDb_director_ID'].map(lambda x: \", \".join(dict_name['Name'].get(i, 'NAN') for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abbd9a0-bc85-4336-aba2-3f43f21d0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['birthYear producer'] = IMDb['IMDb_director_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['birthYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8029a1c8-b400-4071-a2dd-9520ac083a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['deathYear producer'] = IMDb['IMDb_director_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['deathYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd17337c-4c00-4c6e-aa7e-9e27efdd9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['Writer name'] = IMDb['IMDb_writers_ID'].map(lambda x: \", \".join(dict_name['Name'].get(i, 'NAN') for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3020adbd-a874-483a-bd9a-d9bc79776395",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['birthYear writer'] = IMDb['IMDb_writers_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['birthYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb889c5-e565-4763-8034-630cebabf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['deathYear writer'] = IMDb['IMDb_writers_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['deathYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe1961-f1dd-492f-89b6-012948cf5f66",
   "metadata": {},
   "source": [
    "## CMU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb231a69-7e92-414b-91e0-0a385283066c",
   "metadata": {},
   "source": [
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provides also a description of the datasets. A more detailed description figures is in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab0d34-973d-4a64-b9d9-06847e22de11",
   "metadata": {},
   "source": [
    "First, we are interested in the movie.metadata file, which contains information for 81741 movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c00601-84b0-4c04-bba2-21085573672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "NAMES_MOVIES = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_name','Movie_release_date','Movie_box_office_revenue','Movie_runtime','Movie_languages','Movie_countries','Movie_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2247d48-fe66-4090-beb5-9d775192cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_CMU_movies = pd.read_csv(path+\"MovieSummaries/movie.metadata.tsv\", sep='\\t', names = NAMES_MOVIES, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f44b7-8fcb-444c-b46f-5a0a8a6de8c9",
   "metadata": {},
   "source": [
    "Columns \"Movie_genres\", \"Movie_countries\", and \"Movie_languages\" have a dictionary format containing the ID. Let's keep only the values and join each element by a \",\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c7a7beb-255d-4bcb-b31f-93b3f5cecb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CMU_movies[\"Movie_languages\"] = df_CMU_movies[\"Movie_languages\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)\n",
    "df_CMU_movies[\"Movie_countries\"] = df_CMU_movies[\"Movie_countries\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)\n",
    "df_CMU_movies[\"Movie_genres\"] = df_CMU_movies[\"Movie_genres\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2df9b-2033-43ba-994a-5005ae8b0403",
   "metadata": {},
   "source": [
    "## Merging CMU \"movies\" and IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d79b8e-01fd-43d0-947d-9c3a19d7d3e0",
   "metadata": {},
   "source": [
    "In this section, we present two different methods for matching the unique “Wikipedia ID” of movies in the CMU “movies” dataset with the unique “IMDb ID” of movies in the IMDb datasets. This allows us to establish a link between the CMU and IMDb datasets, which will be useful in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4b09-c009-4ca0-997f-ea12d0dd9b40",
   "metadata": {},
   "source": [
    "### Using unique index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f507e-b7f0-452e-805d-b1f7d51b22e9",
   "metadata": {},
   "source": [
    "We will try to merge both datasets using a unique index. Few combinations of columns have been tried to achieve a satisfying result. Here we show our final choice where the new index is composed of the name and the release year of the movies. So we supposed that these 2 pieces of information were necessary to distinguish between every movie. As we will see in the following cells it was sometimes not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18788b06-8145-4199-b7f5-689176460028",
   "metadata": {},
   "source": [
    "To compose our unique index, first we keep only the year as release date for both CMU and IMDb datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9db008c-74c4-4e04-842c-2bd3a22a83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies[\"Movie_release_date\"]=pd.to_datetime(df_CMU_movies[\"Movie_release_date\"], format='mixed', errors='coerce').dt.year.astype('Int64')\n",
    "# For IMDb\n",
    "IMDb['release_date']=pd.to_datetime(IMDb['release_date'], format='mixed', errors='coerce').dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc599a3-720a-44b6-a36d-a6defc3f3a0e",
   "metadata": {},
   "source": [
    "A new column \"modified name\" is created where we drop the punctuation and the space of the movie's name. All characters are also in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f900ca60-7369-4a46-ba78-b554e32d717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies_modified_title = df_CMU_movies.Movie_name.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "df_CMU_movies[\"modified_title\"]=df_CMU_movies_modified_title\n",
    "# For IMDb\n",
    "df_IMDb_modified_title = IMDb.Primary_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "IMDb[\"modified_title\"]=df_IMDb_modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be166525-8251-47ce-ab95-3a2cd55dd36f",
   "metadata": {},
   "source": [
    "Make a copy (to be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426816a4-e39a-4c22-ae6d-0a4af590868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_IMDb_copy = IMDb.copy()\n",
    "init_CMU_copy = df_CMU_movies.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea22836-a056-4101-8e1f-95e831de173c",
   "metadata": {},
   "source": [
    "Then, all rows with NaN in Title name or release date are dropped :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c1f679-624c-463b-b616-8bba64ad44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903 rows are lost with this operation in CMU dataset.  / 81740 \n",
      "101355 rows are lost with this operation in IMDb dataset. / 697175\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "s1 = df_CMU_movies.shape[0]\n",
    "df_CMU_movies = df_CMU_movies.dropna(subset=['Movie_release_date'])\n",
    "s2 = df_CMU_movies.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset.  / {s1} ')\n",
    "# For IMDb\n",
    "s1 = IMDb.shape[0]\n",
    "IMDb = IMDb.dropna(subset=['release_date'])\n",
    "s2 = IMDb.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset. / {s1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772ccdf8-2e79-455b-88fc-86ea0ce7b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of the size of the CMU containing the NaN values : 6903\n"
     ]
    }
   ],
   "source": [
    "# Store a dataset with row with NaN as Title or release date\n",
    "# This step is done in the prevision of the section \"Merging using Data from the web\".\n",
    "CMU_with_NAN = init_CMU_copy[~init_CMU_copy.index.isin(df_CMU_movies.index)]\n",
    "print(f'Quick check of the size of the CMU containing the NaN values : {CMU_with_NAN.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203e8f9-ae42-4e5f-bb7f-94a4b94f4037",
   "metadata": {},
   "source": [
    "Defining new index names based on release date and title : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9e8181-f5e8-4eee-a287-d16826b01953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "new_id_CMU = df_CMU_movies.Movie_release_date.astype(str)+df_CMU_movies.modified_title\n",
    "# For IMDb\n",
    "new_id_IMDb = IMDb.release_date.astype(str)+IMDb.modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5baaf-e746-4b62-ab61-8b1dc5acf471",
   "metadata": {},
   "source": [
    "Once we get the new index, we check that this is a unique index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9d7b52a-aba3-4915-b96f-bdde05a1ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU has a unique indexing : False\n",
      "IMDb has a unique indexing : False\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "CMU_movies_newind = df_CMU_movies.copy()\n",
    "CMU_movies_newind.index = new_id_CMU\n",
    "print(f'CMU has a unique indexing : {CMU_movies_newind.index.is_unique}')\n",
    "# For IMDb\n",
    "IMDb_movies_newind = IMDb.copy()\n",
    "IMDb_movies_newind.index = new_id_IMDb\n",
    "print(f'IMDb has a unique indexing : {IMDb_movies_newind.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25090a-7a41-4c89-9ca8-e585d2836159",
   "metadata": {},
   "source": [
    "Unfortunately we have a non unique indexing.<br>Let's count how much rows have the same index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d22bb518-88c7-4104-9484-1a669e4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 rows are lost with this operation in CMU dataset  / 74837 \n",
      "CMU has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "# Removing the duplicates to create a unique dataset\n",
    "mask_duplicate = CMU_movies_newind.index.duplicated(keep=False)\n",
    "df_CMU_movies_wo_dupl = CMU_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = CMU_movies_newind.shape[0]\n",
    "s2 = df_CMU_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset  / {s1} ')\n",
    "print(f'CMU has a unique indexing : {df_CMU_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8d5cf5c-fbfa-4c50-a5ba-c13677531b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11140 rows are lost with this operation in IMDb dataset / 595820\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "#For IMDb\n",
    "# Removing the duplicates to create a unique dataset\n",
    "mask_duplicate = IMDb_movies_newind.index.duplicated(keep=False)\n",
    "df_IMDb_movies_wo_dupl = IMDb_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = IMDb_movies_newind.shape[0]\n",
    "s2 = df_IMDb_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {df_IMDb_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c738572-94a9-4827-bb36-03306b32124d",
   "metadata": {},
   "source": [
    "As the amount of duplicates is small compared to the original size (<0.5% for CMU and <2% for IMDb), we decided to drop all of them, and not store them somewhere to be reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5f53-12e9-4d44-8ce2-81cfc9bf0364",
   "metadata": {},
   "source": [
    "#### Merge CMU with IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e92e3882-e5c5-4fab-9eba-eca58988d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on our unique index\n",
    "merged = df_IMDb_movies_wo_dupl.merge(df_CMU_movies_wo_dupl, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf6df670-d15c-4f01-92a4-15e9424e3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 44862 out of 81740 have a match\n"
     ]
    }
   ],
   "source": [
    "print(f'A total of {merged.shape[0]} out of {init_CMU_copy.shape[0]} have a match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab8b55-d6c8-4987-86aa-add60819677b",
   "metadata": {},
   "source": [
    "Previously we used the \"primary title\" of the IMDb dataset to create the \"modified_title\" columns. Let's redo the previous operation on the \"original title\" column of the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1fdda-79f0-45d8-ae1d-9945f711e5cd",
   "metadata": {},
   "source": [
    "We don't start with the entire dataset, but we take the unmatched rows of each dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a47a9017-965a-4cc4-a84f-c6107aa6064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550038, 16)\n",
      "(29700, 10)\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU = df_CMU_movies_wo_dupl[~df_CMU_movies_wo_dupl.index.isin(df_IMDb_movies_wo_dupl.index)]\n",
    "notmatched_IMDb = IMDb_movies_newind[~IMDb_movies_newind.index.isin(df_CMU_movies_wo_dupl.index)]\n",
    "\n",
    "print(notmatched_IMDb.shape)\n",
    "print(notmatched_CMU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793bfce4-4159-42d9-85a2-87de5488ff30",
   "metadata": {},
   "source": [
    "(Same procedure as before but with Original_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ec271f7-25be-46b4-bc0a-4bf8c0b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11140 rows are lost with this operation in IMDb dataset / 595820\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "notmatched_IMDb = notmatched_IMDb.drop(columns=[\"modified_title\"])\n",
    "df_IMDb_modified_titlev2 = notmatched_IMDb.Original_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "notmatched_IMDb[\"modified_title\"]=df_IMDb_modified_titlev2\n",
    "new_id_IMDbv2 = notmatched_IMDb.release_date.astype(str)+notmatched_IMDb.modified_title\n",
    "\n",
    "notmatched_IMDb_newind = notmatched_IMDb.copy()\n",
    "notmatched_IMDb_newind.index = new_id_IMDbv2\n",
    "\n",
    "mask_duplicate = notmatched_IMDb_newind.index.duplicated(keep=False)\n",
    "notmatched_IMDb_wo_dupl = notmatched_IMDb_newind[~mask_duplicate]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {notmatched_IMDb_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f110e-179b-42dd-be50-d35ba021f2db",
   "metadata": {},
   "source": [
    "We merge the 2 datasets of unmatched rows (that also have unique index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e5a0143-cd50-44ec-a3b1-aaf8a97bbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We add 3117 rows using the original title !\n"
     ]
    }
   ],
   "source": [
    "mergedv2 = notmatched_IMDb_wo_dupl.merge(notmatched_CMU, left_index=True, right_index=True, how='inner')\n",
    "print(f'We add {mergedv2.shape[0]} rows using the original title !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f2806-97e4-44a1-a184-97640396e7ba",
   "metadata": {},
   "source": [
    "We then concat both merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ddacbb8-05e2-4671-85d6-c81fe8d8c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge dataset has unique indexing : True\n",
      "Size of dataset : (47979, 26)\n"
     ]
    }
   ],
   "source": [
    "merge_final = pd.concat([merged,mergedv2])\n",
    "print(f'Merge dataset has unique indexing : {merge_final.index.is_unique}')\n",
    "print(f'Size of dataset : {merge_final.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759971de-fe3e-41b8-b31f-b9992a74a77a",
   "metadata": {},
   "source": [
    "As we can see, a non-negligible part of the CMU dataset did not find a match. <br> Below, we are concatenating all the unmatched rows from the previous steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de5287c9-218e-41b4-a155-38d5aee1207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched rows of CMU dataset : (33486, 10)\n"
     ]
    }
   ],
   "source": [
    "#CMU\n",
    "# Unmatch from the merging with a unique index\n",
    "notmatched_CMU2 = notmatched_CMU[~notmatched_CMU.index.isin(notmatched_IMDb_wo_dupl.index)]\n",
    "# add also the part of CMU that was dropped because they had a NaN in column \"title or release date\"\n",
    "notmatched_CMU2 = pd.concat([CMU_with_NAN,notmatched_CMU2])\n",
    "\n",
    "#IMDb\n",
    "# Unmatch from the merging with unique index\n",
    "notmatched_IMDb2 = notmatched_IMDb_wo_dupl[~notmatched_IMDb_wo_dupl.index.isin(notmatched_CMU.index)]\n",
    "\n",
    "print(f'Number of unmatched rows of CMU dataset : {notmatched_CMU2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542950d-e94a-423d-978d-6d2391f4a174",
   "metadata": {},
   "source": [
    "As we said previously, the amount of duplicates respectively to their index (release year + name) is almost negligible, we are not adding them in the \"notmatched_CMU2\" and \"notmatched_IMDb2\" for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c37f5f-e562-4bab-b755-92494e15c97a",
   "metadata": {},
   "source": [
    "The next section will try another way to merge these two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd57e-37a0-4ed1-9820-e66097e2e5fa",
   "metadata": {},
   "source": [
    "### Merging using Data from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631bfc0-aa48-4832-b03f-afee596dcc07",
   "metadata": {},
   "source": [
    "We try now to get the IMDb ID of the movie contained in the CMU \"movies\" dataset by scraping Wikipedia. This method seems to be really precise in merging both datasets, but it was not considered in the first step due to its computational time. Now that we've reduced the number of samples, let's see what we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24d6a2-e93b-4387-9f5d-07e964ccbdca",
   "metadata": {},
   "source": [
    "The function \"get_IMDb_id\" do :\n",
    "- go to Wikipedia page of the film using the Wikipedia ID\n",
    "- search in the HTML text for a URL that starts with \"https://www.imdb.com/title/tt\". (This is the IMBb URL of the film)\n",
    "- if there is a single address looking like this on the HTML page it returns the last part of the URL (\"A part of the path\") that corresponds to the IMDb ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b58e7154-0b4d-4406-9a18-f127f4cdf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMDb_id(wikipedia_id):\n",
    "    # sources : \n",
    "    # - https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/\n",
    "    # - https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\n",
    "    r = requests.get(\"https://en.wikipedia.org/?curid=\"+str(wikipedia_id))\n",
    "    \n",
    "    if r.status_code == 404:\n",
    "        return pd.NA\n",
    "        \n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    nbr_link = 0\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile(\"^https://www.imdb.com/title/tt\")}):\n",
    "        nbr_link += 1\n",
    "        href  = link.get('href')\n",
    "        url_parts = url.urlparse(href)\n",
    "        IMBd_id = url_parts.path.split('/')[2]\n",
    "    if nbr_link==1:\n",
    "        return IMBd_id\n",
    "    else :\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e45c6ccb-ed6c-47f8-8fd5-77d97fc36775",
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatched_CMU2_copy = notmatched_CMU2.copy()\n",
    "notmatched_CMU2_copy = notmatched_CMU2_copy['Wikipedia_movie_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df7b56d2-81c6-4196-9328-2a43dcb9f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33486/33486 [8:12:29<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "notmatched_CMU2_copy = notmatched_CMU2_copy.progress_apply(lambda x: get_IMDb_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad2326-bd0c-48ed-9db2-8e83d963b72d",
   "metadata": {},
   "source": [
    "Finally, we can merge the not-matched dataset and concat it with the merge_dataset that we got using a unique indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5da9b056-124a-4758-8212-512d3da28b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13063 new samples have been matched with web scraping !\n",
      "Final score :\n",
      "61042 out of 81740 have been matched\n"
     ]
    }
   ],
   "source": [
    "# Merging the CMU and IMDb from the \"Data from the web\" method\n",
    "notmatched_CMU2.loc[:,'IMDb_title_ID'] = notmatched_CMU2_copy\n",
    "mergev3 = pd.merge(notmatched_IMDb2, notmatched_CMU2, how = 'inner', on = 'IMDb_title_ID' )\n",
    "print(f'{mergev3.shape[0]} new samples have been matched with web scraping !')\n",
    "# Concatenate to create our final dataset, which contains the CMU \"movies\" dataset\n",
    "# with the corresponding / matching IMDb ID \n",
    "final = pd.concat([merge_final,mergev3])\n",
    "print(\"Final score :\")\n",
    "print(f'{final.shape[0]} out of {init_CMU_copy.shape[0]} have been matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cd0a2ed-f3fc-4466-9fd3-4785423dd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put every NaN in the same format\n",
    "final.replace(np.nan, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585496c3-2ae4-4286-9b00-f34bef69a282",
   "metadata": {},
   "source": [
    "## Match ethnicity freebase ID with his english name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6ae6f-9982-49d0-b56d-14face622e04",
   "metadata": {},
   "source": [
    "In this section, we investigate what the freebase ID for ethnicity in the CMU “character” dataset correspond to in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125c867-c747-40d7-a061-8e8e78fdd08c",
   "metadata": {},
   "source": [
    "### Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0711932-b905-43ac-b7ad-014bc723f8a8",
   "metadata": {},
   "source": [
    "Loading character dataset: <br> It's a pickle file but it is exactly the raw dataset \"character.metadata.tsv\" (in a different format) downloaded from CMU datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32bf0677-5d04-4e11-ae6e-7d607059a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_column = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_release_date','Character_name','actor_DOB','actor_gender','actor_height','actor_ethnicity','actor_name','actor_age_atmovierelease','freebase_character_actor_map_id','freebase_character_id','Freebase_actor_ID']\n",
    "characters = pd.read_csv(path+\"MovieSummaries/character.metadata.tsv\", sep='\\t',names = character_column, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4f8c4-95b2-4ab1-8420-59b2bb859b9b",
   "metadata": {},
   "source": [
    "CMU \"character\" dataset contains only the freebase ID of the ethnicity. Then, we have to match the id with the ethnicity.<br> In order to do it we will scrape data from the web. Here are the steps that we follow :\n",
    "- make research in Wikidata for an ethnicity ID\n",
    "- get the HTML text of the research\n",
    "- look at the first URL associated with this research (we suppose that the object of the research (the id) is sufficiently \"precise\" to have only one URL)\n",
    "- finally, we get the label of the corresponding itemlink (which corresponds to the class_ = wb-itemlink-label in Wikidata)\n",
    "\n",
    "<br> These steps are done using the function \"get_ethnie()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7edd9c12-16c8-4035-98f4-de0693959678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ethnie(ethnie_id):\n",
    "\n",
    "    r = requests.get(\"https://www.wikidata.org/wiki/Special:Search\"+ethnie_id)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    # interested in the first link : \"data-serp-pos :0 \".\n",
    "    etnies = soup.find('a', href=True, attrs={'data-serp-pos': '0', 'title': True})\n",
    "    if etnies is not None:\n",
    "        # get the label of the link : \"wb-itemlink-label\"\n",
    "        abc = etnies.find('span', class_='wb-itemlink-label').text\n",
    "    else :\n",
    "        # return 0 if nothing is found in wikidata\n",
    "        abc = 0\n",
    "    return abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58549-69ab-4d8b-84be-a7815286f222",
   "metadata": {},
   "source": [
    "Creating a numpy array that contains all unique ID of the ethnicities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee74d6f0-bd64-4a3d-a889-923ec5cfd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie_id = pd.unique(characters['actor_ethnicity'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1b9fa-54ce-407e-aae3-5eecca0992e1",
   "metadata": {},
   "source": [
    "Creating a series to store the ethnicity names :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a70a722f-3a12-427e-9392-0cdbbeb8a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie = pd.Series(\"ethnie\", index = ethnie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2b496b5-a3a1-4f93-a31a-834fa99bb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie.name = \"ethnicity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b6430-9935-46b3-9498-20fc8db317ab",
   "metadata": {},
   "source": [
    "Scrape data from web as described before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8daa63f6-36a9-4fc0-bc78-d6b7872d4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 479/479 [08:57<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 48 ethnicities id have not found a match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in tqdm(ethnie_id):\n",
    "    ethnie.iloc[a] = get_ethnie(i)\n",
    "    a +=1\n",
    "print(f'A total of {ethnie[ethnie == 0].count()} ethnicities id have not found a match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca285288-32c3-4fa7-8f8d-e29d2d7c97f6",
   "metadata": {},
   "source": [
    "As this method is not totally reliable, we visually check all the ethnicities and adjust their values after searching for the Freebase key online. <br> Here below is the list of ethnicities that we found strange and modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "479b30d0-76f2-419a-ae2c-813f8d170132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie[\"/m/0x67\"] = \"African Americans\"\n",
    "ethnie[\"/m/01trsl\"] = \"First Nations\"\n",
    "ethnie[\"/m/05748\"] = \"Maori\"\n",
    "ethnie[\"/m/059_w\"] = \"Native Americans\"\n",
    "ethnie[\"/m/04zjjt\"] = 0\n",
    "ethnie[\"/m/059v8\"] = \"Nez Perce\"\n",
    "ethnie[\"/m/01d7kx\"] = \"Swedish-speaking population of Finland\"\n",
    "ethnie[\"/m/04_hr_\"] = \"Chinese Singaporeans\"\n",
    "ethnie[\"/m/064pj\"] = \"Persians\"\n",
    "ethnie[\"/m/05ms3p0\"] = \"Punjabi diaspora\"\n",
    "ethnie[\"/m/09snp5\"] = \"Muhajir\"\n",
    "ethnie[\"/m/09cd0m\"] = \"Filipino mestizo\"\n",
    "ethnie[\"/m/03x1x\"] = \"Iroquois\"\n",
    "ethnie[\"/m/012fh\"] = \"Afrikaners\"\n",
    "ethnie[\"/m/05vhv7\"] = 0\n",
    "ethnie[\"/m/04c28\"] = \"Kurds\"\n",
    "ethnie[\"/m/03sk2\"] = 0\n",
    "ethnie[\"/m/0xff\"] = \"Arabs\"\n",
    "ethnie[\"/m/03cdk7b\"] = \"British Pakistanis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd41227-6b1a-4f01-8c1a-547b34f7e73e",
   "metadata": {},
   "source": [
    "The “0” values correspond to the fact that the ID is not known on Wikidata. We replace all 0's with NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d703608-528e-49e6-8061-2aa7cffd66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie.replace(0, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b327b-53de-46d6-9b2b-9b4d25f14ac9",
   "metadata": {},
   "source": [
    "Now, we can merge the CMU \"character\" dataset with the \"complete ethnicity\" dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55dbf859-f908-4ce6-9886-8ee301988e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.merge(characters, ethnie, left_on = 'actor_ethnicity', right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "301f998d-a8f9-4a7f-89d0-ee0e5b2c4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put every NaN in the same format\n",
    "characters.replace(np.nan, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970661b-1557-41a6-af29-7a2f4887b032",
   "metadata": {},
   "source": [
    "## Modifying CMU \"character\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f054728-8401-4d28-bfd9-d88ad49c6bf4",
   "metadata": {},
   "source": [
    "We are modifying the final CMU \"character\" dataset for the needs of our project. Indeed, the CMU \"character\" dataset was aligned with the movies, and for our project, we need to group by the actors, and not the movies. To do so, we create a new dataframe called \"actors\", which will group an actor with all the info we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd1f40e6-7afb-4fa5-8251-2c88029a0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actors = (\n",
    "    characters.groupby(\"Freebase_actor_ID\")\n",
    "    .agg({\n",
    "        \"actor_name\": \"first\",  \n",
    "        \"actor_DOB\": \"first\",  \n",
    "        \"actor_gender\": \"first\",  \n",
    "        \"actor_height\": \"mean\",\n",
    "        \"actor_ethnicity\": \"first\",\n",
    "        \"Freebase_movie_ID\": list, \n",
    "        \"actor_age_atmovierelease\": list, \n",
    "    })\n",
    "    .reset_index() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f75c0-a037-49e8-92b7-0a0eabbb7946",
   "metadata": {},
   "source": [
    "## Save final dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eab40-a163-414a-ac8a-4b9dfb2a3474",
   "metadata": {},
   "source": [
    "All datasets are ready to be used for the analysis. Let's save them as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdd8c830-a162-400c-8f9a-75a9769cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/Users/alexandre/Desktop/Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec2f1d5a-778c-4c4e-bed0-f525e7869691",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.reset_index().to_pickle(path+'Movie.pkl')\n",
    "Actors.to_pickle(path+'Actor.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
