{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1329e0f-fddc-4608-b031-820efc996276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import collections\n",
    "\n",
    "from src.utils.data_utils import *\n",
    "from src.utils.general_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3ea1bb-baeb-4912-9d62-3a12c81cf956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_7236\\2221900138.py:6: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  Movie = pickle.load(file)\n"
     ]
    }
   ],
   "source": [
    "movie_path = 'data/Movie.pkl'\n",
    "actors_path = 'data/Actor.pkl'\n",
    "actors_career_path = 'data/Actor_career.pkl'\n",
    "\n",
    "with open(movie_path, 'rb') as file:\n",
    "    Movie = pickle.load(file)\n",
    "\n",
    "with open(actors_path, 'rb') as file:\n",
    "    Actor = pickle.load(file)\n",
    "\n",
    "with open(actors_career_path, 'rb') as file:\n",
    "    Actor_career = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bf919e5f-c8af-4b40-9927-9f8b9944c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actor[\"actor_age_atmovierelease\"] = Actor[\"actor_age_atmovierelease\"].apply(\n",
    "    lambda x: [pd.NA if pd.isna(age) else int(age) for age in x]\n",
    ")\n",
    "\n",
    "Actor[\"actor_age_atmovierelease\"] = Actor[\"actor_age_atmovierelease\"].apply(\n",
    "    lambda x: [-1 if (pd.isna(val) or val < 0) else val for val in x]\n",
    ")\n",
    "\n",
    "Actor[\"Career_Start_age\"] = Actor.apply(\n",
    "    lambda row: min([val for val in row[\"actor_age_atmovierelease\"] if val > 0], default=-1),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "max_career_years = int(max(Actor.apply(lambda row: max(row[\"actor_age_atmovierelease\"]),axis=1)-Actor[\"Career_Start_age\"]))\n",
    "\n",
    "Nbr_films = [f\"Nbr_films_{i+1}\" for i in range(100)]\n",
    "Degree = [f\"Degree_{i+1}\" for i in range(100)]\n",
    "Centrality = [f\"Centrality_{i+1}\" for i in range(100)]\n",
    "\n",
    "\n",
    "Actor_career = pd.concat(\n",
    "    [Actor.copy(), pd.DataFrame(0.0, index=Actor.index, columns=Nbr_films), pd.DataFrame(0.0, index=Actor.index, columns=Degree), pd.DataFrame(0.0, index=Actor.index, columns=Centrality)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "56923b16-00d8-4f5a-b823-f6ab67306231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_career_years(row):\n",
    "\n",
    "    age_at_release = np.array(row[\"actor_age_atmovierelease\"])\n",
    "    counts = np.zeros(max_career_years+1)\n",
    "    \n",
    "    if len(age_at_release) != 0:\n",
    "        for year in age_at_release:\n",
    "            if year >= 0 and row[\"Career_Start_age\"]>=0:\n",
    "                counts[year-row[\"Career_Start_age\"]] += 1\n",
    "                    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d1314be7-eea4-4190-87c9-c607f390121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13293.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1907/2024) of 11 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 11/11 [00:00<00:00, 4967.95it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13625.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1908/2024) of 15 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 15/15 [00:00<00:00, 5005.93it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 15566.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1909/2024) of 21 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 21/21 [00:00<00:00, 4901.25it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:03<00:00, 16494.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1910/2024) of 28 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 28/28 [00:00<00:00, 7002.18it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13018.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1911/2024) of 47 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 47/47 [00:00<00:00, 7819.61it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12476.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1912/2024) of 94 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 94/94 [00:00<00:00, 6268.22it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:03<00:00, 16196.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1913/2024) of 157 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 157/157 [00:00<00:00, 6281.41it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12554.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1914/2024) of 231 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 231/231 [00:00<00:00, 6966.18it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 16005.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1915/2024) of 397 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 397/397 [00:00<00:00, 6884.64it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12342.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1916/2024) of 644 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 644/644 [00:00<00:00, 7198.87it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12434.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1917/2024) of 881 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 881/881 [00:00<00:00, 7394.79it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:03<00:00, 16382.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1918/2024) of 1103 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 1103/1103 [00:00<00:00, 7560.84it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 14856.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1919/2024) of 1332 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 1332/1332 [00:00<00:00, 6689.36it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12398.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1920/2024) of 1532 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 1532/1532 [00:00<00:00, 7187.67it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:03<00:00, 16189.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1921/2024) of 1831 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 1831/1831 [00:00<00:00, 6471.94it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 15714.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1922/2024) of 2114 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 2114/2114 [00:00<00:00, 6860.15it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12429.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1923/2024) of 2348 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 2348/2348 [00:00<00:00, 7394.56it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 14436.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1924/2024) of 2652 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 2652/2652 [00:00<00:00, 3790.83it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13516.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1925/2024) of 2884 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 2884/2884 [00:00<00:00, 8078.86it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11254.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1926/2024) of 3147 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 3147/3147 [00:00<00:00, 4971.46it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13780.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1927/2024) of 3384 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 3384/3384 [00:00<00:00, 8246.26it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 9600.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1928/2024) of 3704 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 3704/3704 [00:00<00:00, 7915.94it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12688.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1929/2024) of 4003 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 4003/4003 [00:00<00:00, 8485.17it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13178.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1930/2024) of 4424 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 4424/4424 [00:00<00:00, 8094.12it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11963.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1931/2024) of 4917 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 4917/4917 [00:00<00:00, 7838.66it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12389.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1932/2024) of 5387 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 5387/5387 [00:00<00:00, 8383.57it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 14330.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1933/2024) of 5785 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 5785/5785 [00:00<00:00, 5901.55it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13439.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1934/2024) of 6131 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 6131/6131 [00:00<00:00, 6561.15it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8777.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1935/2024) of 6656 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 6656/6656 [00:01<00:00, 5059.99it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4757.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1936/2024) of 7131 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 7131/7131 [00:02<00:00, 2869.76it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6576.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1937/2024) of 7735 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 7735/7735 [00:03<00:00, 2178.55it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6971.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1938/2024) of 8264 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 8264/8264 [00:02<00:00, 2846.35it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4794.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1939/2024) of 8704 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 8704/8704 [00:04<00:00, 2103.87it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 6089.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1940/2024) of 9125 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 9125/9125 [00:03<00:00, 2896.99it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5640.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1941/2024) of 9531 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 9531/9531 [00:02<00:00, 3255.98it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 5850.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1942/2024) of 9923 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 9923/9923 [00:07<00:00, 1404.60it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 6320.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1943/2024) of 10267 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 10267/10267 [00:01<00:00, 6147.66it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:04<00:00, 13097.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1944/2024) of 10652 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 10652/10652 [00:01<00:00, 6896.59it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 9515.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1945/2024) of 11051 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 11051/11051 [00:03<00:00, 3453.27it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6866.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1946/2024) of 11380 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 11380/11380 [00:01<00:00, 7885.27it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 9558.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1947/2024) of 11785 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 11785/11785 [00:01<00:00, 8272.33it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8309.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1948/2024) of 12188 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 12188/12188 [00:01<00:00, 7713.53it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11584.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1949/2024) of 12649 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 12649/12649 [00:02<00:00, 6238.89it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11700.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1950/2024) of 13284 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 13284/13284 [00:01<00:00, 7855.11it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11901.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1951/2024) of 13962 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 13962/13962 [00:01<00:00, 7973.27it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 12037.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1952/2024) of 14650 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 14650/14650 [00:02<00:00, 6847.57it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11917.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1953/2024) of 15348 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 15348/15348 [00:02<00:00, 6315.72it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11573.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1954/2024) of 16025 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 16025/16025 [00:02<00:00, 6251.28it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 11613.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1955/2024) of 16644 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 16644/16644 [00:02<00:00, 6465.16it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 10434.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1956/2024) of 17331 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 17331/17331 [00:03<00:00, 4919.10it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 9875.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1957/2024) of 18071 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 18071/18071 [00:02<00:00, 7927.03it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 10915.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1958/2024) of 18844 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 18844/18844 [00:03<00:00, 6146.61it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:05<00:00, 10884.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1959/2024) of 19545 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 19545/19545 [00:02<00:00, 6904.17it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8388.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1960/2024) of 20294 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 20294/20294 [00:02<00:00, 7924.16it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 9034.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1961/2024) of 21066 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 21066/21066 [00:02<00:00, 7880.02it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8857.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1962/2024) of 21855 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 21855/21855 [00:04<00:00, 4733.90it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8703.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1963/2024) of 22703 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 22703/22703 [00:03<00:00, 7488.01it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7925.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1964/2024) of 23391 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 23391/23391 [00:03<00:00, 7510.52it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8211.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1965/2024) of 24278 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 24278/24278 [00:03<00:00, 7925.17it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7644.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1966/2024) of 25195 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 25195/25195 [00:03<00:00, 7748.60it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8266.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1967/2024) of 25980 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 25980/25980 [00:05<00:00, 4977.03it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7734.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1968/2024) of 26873 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 26873/26873 [00:03<00:00, 7910.49it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 9119.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1969/2024) of 27872 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 27872/27872 [00:04<00:00, 6421.54it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8095.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1970/2024) of 28921 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 28921/28921 [00:04<00:00, 6226.08it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:06<00:00, 9518.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1971/2024) of 30179 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 30179/30179 [00:03<00:00, 7845.41it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 6253.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1972/2024) of 31394 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 31394/31394 [00:03<00:00, 7995.52it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8606.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1973/2024) of 32604 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 32604/32604 [00:04<00:00, 6947.39it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6718.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1974/2024) of 33763 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 33763/33763 [00:05<00:00, 6321.85it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 5901.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1975/2024) of 34852 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 34852/34852 [00:05<00:00, 6128.92it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 5942.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1976/2024) of 35779 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 35779/35779 [00:05<00:00, 6065.78it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7375.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1977/2024) of 36833 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 36833/36833 [00:05<00:00, 6301.53it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7455.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1978/2024) of 37888 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 37888/37888 [00:05<00:00, 6455.08it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7419.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1979/2024) of 38980 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 38980/38980 [00:09<00:00, 4204.38it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8133.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1980/2024) of 40239 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 40239/40239 [00:05<00:00, 7623.77it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8282.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1981/2024) of 41563 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 41563/41563 [00:05<00:00, 7804.00it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:07<00:00, 8240.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1982/2024) of 42885 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 42885/42885 [00:06<00:00, 6656.21it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4688.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1983/2024) of 44201 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 44201/44201 [00:06<00:00, 7347.88it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:08<00:00, 7827.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1984/2024) of 45392 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 45392/45392 [00:06<00:00, 6712.77it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6979.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1985/2024) of 46831 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 46831/46831 [00:06<00:00, 7041.68it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4919.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1986/2024) of 48265 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 48265/48265 [00:08<00:00, 6005.22it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6846.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1987/2024) of 49670 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 49670/49670 [00:06<00:00, 7363.88it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5499.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1988/2024) of 51357 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 51357/51357 [00:08<00:00, 6207.60it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6527.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1989/2024) of 52831 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 52831/52831 [00:06<00:00, 7763.76it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6786.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1990/2024) of 54217 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 54217/54217 [00:07<00:00, 6855.24it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:09<00:00, 6490.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1991/2024) of 55580 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 55580/55580 [00:07<00:00, 7358.41it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 6358.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1992/2024) of 57024 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 57024/57024 [00:08<00:00, 6721.47it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5568.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1993/2024) of 58466 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 58466/58466 [00:10<00:00, 5487.56it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4829.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1994/2024) of 59902 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 59902/59902 [00:08<00:00, 6843.52it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 6369.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1995/2024) of 61555 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 61555/61555 [00:11<00:00, 5355.88it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:12<00:00, 5091.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1996/2024) of 63148 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 63148/63148 [00:08<00:00, 7758.08it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:10<00:00, 5946.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1997/2024) of 64958 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 64958/64958 [00:13<00:00, 4805.59it/s]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5702.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1998/2024) of 66848 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 66848/66848 [00:08<00:00, 7811.64it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4864.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (1999/2024) of 68922 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 68922/68922 [00:09<00:00, 7461.31it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5487.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2000/2024) of 71179 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 71179/71179 [00:12<00:00, 5678.44it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5399.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2001/2024) of 73793 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 73793/73793 [00:10<00:00, 7020.81it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4639.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2002/2024) of 76784 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 76784/76784 [00:09<00:00, 7745.00it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:18<00:00, 3551.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2003/2024) of 79860 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 79860/79860 [00:11<00:00, 6718.68it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:11<00:00, 5468.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2004/2024) of 82934 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 82934/82934 [00:15<00:00, 5295.25it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4678.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2005/2024) of 86847 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 86847/86847 [00:11<00:00, 7563.55it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:15<00:00, 4253.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2006/2024) of 90696 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 90696/90696 [00:12<00:00, 7096.57it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:15<00:00, 4128.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2007/2024) of 95608 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 95608/95608 [00:16<00:00, 5761.62it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:13<00:00, 4653.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2008/2024) of 100226 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 100226/100226 [00:14<00:00, 7007.57it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:16<00:00, 3835.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2009/2024) of 105001 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 105001/105001 [00:14<00:00, 7179.97it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:14<00:00, 4379.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2010/2024) of 109676 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 109676/109676 [00:16<00:00, 6759.03it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:15<00:00, 4225.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2011/2024) of 113918 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 113918/113918 [00:19<00:00, 5806.00it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:18<00:00, 3564.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2012/2024) of 117717 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 117717/117717 [00:17<00:00, 6729.88it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:18<00:00, 3571.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2013/2024) of 119113 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119113/119113 [00:16<00:00, 7115.29it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:22<00:00, 2920.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2014/2024) of 119251 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119251/119251 [00:18<00:00, 6338.51it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:16<00:00, 3842.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2015/2024) of 119320 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119320/119320 [00:16<00:00, 7019.26it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:16<00:00, 4005.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2016/2024) of 119339 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119339/119339 [00:17<00:00, 6915.95it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:16<00:00, 3966.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2017/2024) of 119348 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119348/119348 [00:22<00:00, 5417.17it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:16<00:00, 3939.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2018/2024) of 119353 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119353/119353 [00:18<00:00, 6602.09it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:18<00:00, 3472.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2019/2024) of 119364 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119364/119364 [00:16<00:00, 7196.23it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
      "Creating network: 100%|██████████| 64308/64308 [00:21<00:00, 2949.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network (2020/2024) of 119365 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill the dataset: 100%|██████████| 119365/119365 [00:19<00:00, 6170.42it/s]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
      "C:\\Users\\pierr\\AppData\\Local\\Temp\\ipykernel_10352\\2840603769.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 4317.73s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "movie_releasedates = range(1920,2020) \n",
    "Actor_career['carrer_year'] = 0\n",
    "career_counts = np.array(Actor_career.apply(fill_career_years, axis=1).tolist())\n",
    "\n",
    "for i in range(max_career_years):\n",
    "    Actor_career[f\"Year_{i+1}\"] = career_counts[:, i]\n",
    "    Actor_career[f\"Nbr_films_{i+1}\"] = career_counts[:, i]\n",
    "\n",
    "\n",
    "for date in movie_releasedates:\n",
    "\n",
    "    played_together = create_actor_network(Actor_career, Movie, 1, min_releasedate=date)\n",
    "\n",
    "    print(f\"Network ({date+1}/2024) of {played_together.number_of_nodes()} nodes.\")\n",
    "\n",
    "    nan_nodes = [node for node in played_together.nodes if pd.isna(node)]\n",
    "    played_together.remove_nodes_from(nan_nodes)\n",
    "\n",
    "    degrees = dict(played_together.degree())\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(played_together, max_iter=1000)\n",
    "    eigenvector_values = np.array(list(eigenvector_centrality.values()))\n",
    "\n",
    "    mean = np.mean(eigenvector_values)\n",
    "    std = np.std(eigenvector_values)\n",
    "    standardized_eigenvector_centrality = {node: (val - mean) / std for node, val in eigenvector_centrality.items()}\n",
    "\n",
    "\n",
    "    actor_id_to_idx = pd.Series(Actor_career.index, index=Actor_career[\"Freebase_actor_ID\"]).to_dict()\n",
    "    num_actors = len(Actor_career)\n",
    "    degree_updates = np.zeros(num_actors, dtype=np.float32)\n",
    "    centrality_updates = np.zeros(num_actors, dtype=np.float32)\n",
    "    career_year_updates = np.zeros(num_actors, dtype=np.int32)\n",
    "\n",
    "\n",
    "    for actor_id in tqdm(played_together.nodes(), desc=\"Fill the dataset\"):\n",
    "        if actor_id in actor_id_to_idx:\n",
    "            actor_idx = actor_id_to_idx[actor_id]\n",
    "            career_year = Actor_career.at[actor_idx, \"carrer_year\"]\n",
    "            \n",
    "            degree_updates[actor_idx] = degrees.get(actor_id, 0)\n",
    "            centrality_updates[actor_idx] = standardized_eigenvector_centrality.get(actor_id, 0.0)\n",
    "            career_year_updates[actor_idx] = career_year\n",
    "\n",
    "            Actor_career.at[actor_idx, \"carrer_year\"] += 1\n",
    "\n",
    "\n",
    "    for actor_idx, career_year in enumerate(career_year_updates):\n",
    "        if career_year > 0:  # Only update valid career years\n",
    "            Actor_career.at[actor_idx, f\"Degree_{career_year}\"] = degree_updates[actor_idx]\n",
    "            Actor_career.at[actor_idx, f\"Centrality_{career_year}\"] = centrality_updates[actor_idx]\n",
    "    \n",
    "print(f\"Computation time: {time.time()-start:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1df5e0e-f50c-43f4-ba8f-18c240ee0aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freebase_actor_ID</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>actor_DOB</th>\n",
       "      <th>actor_gender</th>\n",
       "      <th>actor_height</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>Freebase_movie_ID</th>\n",
       "      <th>actor_age_atmovierelease</th>\n",
       "      <th>Career_Start_age</th>\n",
       "      <th>Nbr_films_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Degree_109</th>\n",
       "      <th>Centrality_109</th>\n",
       "      <th>Degree_110</th>\n",
       "      <th>Centrality_110</th>\n",
       "      <th>Degree_111</th>\n",
       "      <th>Centrality_111</th>\n",
       "      <th>Degree_112</th>\n",
       "      <th>Centrality_112</th>\n",
       "      <th>Degree_113</th>\n",
       "      <th>Centrality_113</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/010p3</td>\n",
       "      <td>Adam Carolla</td>\n",
       "      <td>1964-05-27</td>\n",
       "      <td>M</td>\n",
       "      <td>1.88</td>\n",
       "      <td>Italian Americans</td>\n",
       "      <td>[/m/0bq2wj, /m/08hjr9, /m/06zm9p7, /m/06c5z_, ...</td>\n",
       "      <td>[35, 35, 45, 33, 33, 34, 42, 35, -1, 44, 47, 38]</td>\n",
       "      <td>33</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/010q36</td>\n",
       "      <td>Fred Rogers</td>\n",
       "      <td>1928-03-20</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/033pf1]</td>\n",
       "      <td>[67]</td>\n",
       "      <td>67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/010wx</td>\n",
       "      <td>Aria Giovanni</td>\n",
       "      <td>1977-11-03</td>\n",
       "      <td>F</td>\n",
       "      <td>1.68</td>\n",
       "      <td>Yugoslavs</td>\n",
       "      <td>[/m/07gx0c]</td>\n",
       "      <td>[25]</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/010xjr</td>\n",
       "      <td>Richard Harris</td>\n",
       "      <td>1930-10-01</td>\n",
       "      <td>M</td>\n",
       "      <td>1.85</td>\n",
       "      <td>Irish people</td>\n",
       "      <td>[/m/0c_6bf, /m/078mm1, /m/03qfyt, /m/0cz7_4w, ...</td>\n",
       "      <td>[66, 46, -1, 59, 48, 39, 66, 50, 71, 71, 68, 7...</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/0112yl</td>\n",
       "      <td>Toshiro Mifune</td>\n",
       "      <td>1920-04-01</td>\n",
       "      <td>M</td>\n",
       "      <td>1.74</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/0513nd, /m/05c2g3q, /m/04wjg1, /m/01tch8, ...</td>\n",
       "      <td>[31, 42, 45, 41, 62, 29, 30, 38, 35, 36, 59, 6...</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135755</th>\n",
       "      <td>/m/0z58</td>\n",
       "      <td>Ardal O'Hanlon</td>\n",
       "      <td>1965-10-08</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/027cvd8]</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135756</th>\n",
       "      <td>/m/0z9q0</td>\n",
       "      <td>José Luis Rodríguez \"El Puma\"</td>\n",
       "      <td>1943-01-14</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/0gls5qd]</td>\n",
       "      <td>[69]</td>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135757</th>\n",
       "      <td>/m/0zcb7</td>\n",
       "      <td>Marcus Giamatti</td>\n",
       "      <td>1961-10-03</td>\n",
       "      <td>M</td>\n",
       "      <td>1.85</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/080kkcx, /m/09v460, /m/02drs7, /m/0gls5v_,...</td>\n",
       "      <td>[38, 29, 40, 48, 29, 46]</td>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135758</th>\n",
       "      <td>/m/0zcbl</td>\n",
       "      <td>Paul Giamatti</td>\n",
       "      <td>1967-06-06</td>\n",
       "      <td>M</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Irish Americans</td>\n",
       "      <td>[/m/0ck5rr, /m/04yg13l, /m/04wdfw, /m/03whlgj,...</td>\n",
       "      <td>[38, 43, 28, 41, 46, 43, 28, 31, 30, 37, 24, 3...</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135759</th>\n",
       "      <td>/m/0zjpz</td>\n",
       "      <td>Richie Sambora</td>\n",
       "      <td>1959-07-11</td>\n",
       "      <td>M</td>\n",
       "      <td>1.829</td>\n",
       "      <td>None</td>\n",
       "      <td>[/m/05sqdmg]</td>\n",
       "      <td>[49]</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135760 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Freebase_actor_ID                     actor_name   actor_DOB  \\\n",
       "0               /m/010p3                   Adam Carolla  1964-05-27   \n",
       "1              /m/010q36                    Fred Rogers  1928-03-20   \n",
       "2               /m/010wx                  Aria Giovanni  1977-11-03   \n",
       "3              /m/010xjr                 Richard Harris  1930-10-01   \n",
       "4              /m/0112yl                 Toshiro Mifune  1920-04-01   \n",
       "...                  ...                            ...         ...   \n",
       "135755           /m/0z58                 Ardal O'Hanlon  1965-10-08   \n",
       "135756          /m/0z9q0  José Luis Rodríguez \"El Puma\"  1943-01-14   \n",
       "135757          /m/0zcb7                Marcus Giamatti  1961-10-03   \n",
       "135758          /m/0zcbl                  Paul Giamatti  1967-06-06   \n",
       "135759          /m/0zjpz                 Richie Sambora  1959-07-11   \n",
       "\n",
       "       actor_gender actor_height          ethnicity  \\\n",
       "0                 M         1.88  Italian Americans   \n",
       "1                 M          NaN               None   \n",
       "2                 F         1.68          Yugoslavs   \n",
       "3                 M         1.85       Irish people   \n",
       "4                 M         1.74               None   \n",
       "...             ...          ...                ...   \n",
       "135755            M          NaN               None   \n",
       "135756            M          NaN               None   \n",
       "135757            M         1.85               None   \n",
       "135758            M         1.74    Irish Americans   \n",
       "135759            M        1.829               None   \n",
       "\n",
       "                                        Freebase_movie_ID  \\\n",
       "0       [/m/0bq2wj, /m/08hjr9, /m/06zm9p7, /m/06c5z_, ...   \n",
       "1                                             [/m/033pf1]   \n",
       "2                                             [/m/07gx0c]   \n",
       "3       [/m/0c_6bf, /m/078mm1, /m/03qfyt, /m/0cz7_4w, ...   \n",
       "4       [/m/0513nd, /m/05c2g3q, /m/04wjg1, /m/01tch8, ...   \n",
       "...                                                   ...   \n",
       "135755                                       [/m/027cvd8]   \n",
       "135756                                       [/m/0gls5qd]   \n",
       "135757  [/m/080kkcx, /m/09v460, /m/02drs7, /m/0gls5v_,...   \n",
       "135758  [/m/0ck5rr, /m/04yg13l, /m/04wdfw, /m/03whlgj,...   \n",
       "135759                                       [/m/05sqdmg]   \n",
       "\n",
       "                                 actor_age_atmovierelease  Career_Start_age  \\\n",
       "0        [35, 35, 45, 33, 33, 34, 42, 35, -1, 44, 47, 38]                33   \n",
       "1                                                    [67]                67   \n",
       "2                                                    [25]                25   \n",
       "3       [66, 46, -1, 59, 48, 39, 66, 50, 71, 71, 68, 7...                28   \n",
       "4       [31, 42, 45, 41, 62, 29, 30, 38, 35, 36, 59, 6...                28   \n",
       "...                                                   ...               ...   \n",
       "135755                                               [-1]                -1   \n",
       "135756                                               [69]                69   \n",
       "135757                           [38, 29, 40, 48, 29, 46]                29   \n",
       "135758  [38, 43, 28, 41, 46, 43, 28, 31, 30, 37, 24, 3...                24   \n",
       "135759                                               [49]                49   \n",
       "\n",
       "        Nbr_films_1  ...  Degree_109  Centrality_109  Degree_110  \\\n",
       "0               2.0  ...         NaN             NaN         NaN   \n",
       "1               1.0  ...         NaN             NaN         NaN   \n",
       "2               1.0  ...         NaN             NaN         NaN   \n",
       "3               1.0  ...         NaN             NaN         NaN   \n",
       "4               2.0  ...         NaN             NaN         NaN   \n",
       "...             ...  ...         ...             ...         ...   \n",
       "135755          0.0  ...         NaN             NaN         NaN   \n",
       "135756          1.0  ...         NaN             NaN         NaN   \n",
       "135757          2.0  ...         NaN             NaN         NaN   \n",
       "135758          1.0  ...         NaN             NaN         NaN   \n",
       "135759          1.0  ...         NaN             NaN         NaN   \n",
       "\n",
       "        Centrality_110  Degree_111  Centrality_111  Degree_112  \\\n",
       "0                  NaN         NaN             NaN         NaN   \n",
       "1                  NaN         NaN             NaN         NaN   \n",
       "2                  NaN         NaN             NaN         NaN   \n",
       "3                  NaN         NaN             NaN         NaN   \n",
       "4                  NaN         NaN             NaN         NaN   \n",
       "...                ...         ...             ...         ...   \n",
       "135755             NaN         NaN             NaN         NaN   \n",
       "135756             NaN         NaN             NaN         NaN   \n",
       "135757             NaN         NaN             NaN         NaN   \n",
       "135758             NaN         NaN             NaN         NaN   \n",
       "135759             NaN         NaN             NaN         NaN   \n",
       "\n",
       "        Centrality_112  Degree_113  Centrality_113  \n",
       "0                  NaN         NaN             NaN  \n",
       "1                  NaN         NaN             NaN  \n",
       "2                  NaN         NaN             NaN  \n",
       "3                  NaN         NaN             NaN  \n",
       "4                  NaN         NaN             NaN  \n",
       "...                ...         ...             ...  \n",
       "135755             NaN         NaN             NaN  \n",
       "135756             NaN         NaN             NaN  \n",
       "135757             NaN         NaN             NaN  \n",
       "135758             NaN         NaN             NaN  \n",
       "135759             NaN         NaN             NaN  \n",
       "\n",
       "[135760 rows x 416 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Actor_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb4b6b91-95b0-46a0-9a56-f0042d4974e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(10.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Actor_career['Degree_113'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3c0ad9a7-d2c3-4a69-9bbe-139f617c934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actor_career.to_pickle('data/Actor_career.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91032a47-72f5-4a31-a78c-e532c78ee159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
