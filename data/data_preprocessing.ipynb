{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c35940-0471-47ca-b1ba-690c0d8f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from T_data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca373906-45b3-4e92-8f42-ee56a8a77653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tdqm\n",
      "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm (from tdqm)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: tdqm\n",
      "  Building wheel for tdqm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1322 sha256=76aa6f32e3e6f5e0a60388d3cd909bbd3e1c5530ded69d97c5f6f4db2c773606\n",
      "  Stored in directory: /Users/alexandre/Library/Caches/pip/wheels/c8/c7/30/e5935be2cfa6883be72462333edc414d8928f3c78eaabec38a\n",
      "Successfully built tdqm\n",
      "Installing collected packages: tqdm, tdqm\n",
      "Successfully installed tdqm-0.0.1 tqdm-4.67.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tdqm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f8231-996b-44be-a0a5-3edf522c85bb",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212a1c-6acd-4bef-9252-ee1c9b906a96",
   "metadata": {},
   "source": [
    "This notebook aims to explain the preprocessing steps in order to get the datasets that will be used for the analysis, repectively \"Actor.pkl\" and \"Movie.pkl\" <br> All these steps are indicative, you don't have to run all these cells as running all the notebook a quite time-consuming (around 7 hours due to the part \"Data from Web\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8ec3d-fe0a-4b91-90a5-c6b09992892f",
   "metadata": {},
   "source": [
    "## IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec45fe5-7679-4471-9e06-2a1f5767eac1",
   "metadata": {},
   "source": [
    "First, you can go to https://datasets.imdbws.com, download the files and extract them :\n",
    "- title.basics.tsv.gz\n",
    "- title.crew.tsv.gz\n",
    "- title.ratings.tsv.gz\n",
    "- name.basics.tsv.gz\n",
    "  \n",
    "<br> Note that a descriptive of this dataset can be found on https://developer.imdb.com/non-commercial-datasets/ or in the README in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5029194e-97d3-4be3-9471-c9f0c062ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the path of the folder you have just downloaded\n",
    "# the folder containing all datasets on your machine :\n",
    "path = \"/Users/alexandre/Desktop/Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5f375-f4ea-452b-85eb-8f7fd5b84a9a",
   "metadata": {},
   "source": [
    "Custom column names are defined :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1163a18-8537-46d3-9031-6e4cf1f1f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_CREW = ['IMDb_title_ID','IMDb_director_ID', 'IMDb_writers_ID']\n",
    "NAMES_ratings = ['IMDb_title_ID', 'Average rating', 'number of votes'] \n",
    "NAMES_BASICS = ['IMDb_people_ID', 'Name', 'birthYear', 'deathYear', 'profession', 'knownForTitles']\n",
    "NAMES_TITLES = ['IMDb_title_ID', 'TitleType', 'Primary_title', 'Original_title', 'isAdult', 'release_date', 'end_year', 'runtime', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a24d5-2398-4cbc-8134-88f5a51697b9",
   "metadata": {},
   "source": [
    "All datasets are loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b82ede-362e-4e45-beeb-10b0790bea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_crew = pd.read_csv(path+\"/title.crew.tsv\", sep='\\t', names = NAMES_CREW, header = 0)\n",
    "df_IMDb_ratings = pd.read_csv(path+\"/title.ratings.tsv\", sep='\\t', names = NAMES_ratings, header = 0)\n",
    "df_IMDb_title = pd.read_csv(path+\"/title.basics.tsv\", sep='\\t', names = NAMES_TITLES, header = 0, low_memory=False)\n",
    "df_IMDb_name = pd.read_csv(path+\"/name.basics.tsv\", sep='\\t', names = NAMES_BASICS, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960f395-8efd-427f-9f37-22c696a23df1",
   "metadata": {},
   "source": [
    "For title dataset, only movie must be selected (drop series,...). Without this set, df_IMDb_title is huge.\n",
    "<br> Few unecessary columns are also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dee27ca-0979-469d-986d-3957453816cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_type = 'movie'\n",
    "df_IMDb_title = df_IMDb_title.query('TitleType==@selected_type')\n",
    "df_IMDb_title=df_IMDb_title[['IMDb_title_ID', 'release_date','runtime','Primary_title', 'Original_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984dfdb-6bf8-40ea-827d-007fefeb8c2f",
   "metadata": {},
   "source": [
    "### Merging Titles, crews and ratings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c0683e-902d-4249-8799-30dea5a7325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696953, 5)\n",
      "(1497560, 3)\n",
      "(10563827, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_IMDb_title.shape)\n",
    "print(df_IMDb_ratings.shape)\n",
    "print(df_IMDb_crew.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c24a49-a482-44ff-b0c2-c3d7719ac922",
   "metadata": {},
   "source": [
    "The ratings and crew dataset contains also ratings of non-movie type. We merge on title datset as we have already selected the movie's row. (here, how = left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bc0958-77a5-4ca4-9fc9-ddaffc003508",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_title_rating = pd.merge(df_IMDb_title, df_IMDb_ratings, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de7186c-3c86-45e0-b8aa-7a0c2800761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb = pd.merge(IMDb_title_rating, df_IMDb_crew, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe1961-f1dd-492f-89b6-012948cf5f66",
   "metadata": {},
   "source": [
    "## CMU \"Per title\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb231a69-7e92-414b-91e0-0a385283066c",
   "metadata": {},
   "source": [
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provide also a descriptive of the datasets. A more detailed description figures in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab0d34-973d-4a64-b9d9-06847e22de11",
   "metadata": {},
   "source": [
    "We are interested in the movie.metadata file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c00601-84b0-4c04-bba2-21085573672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "NAMES_MOVIES = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_name','Movie_release_date','Movie_box_office_revenue','Movie_runtime','Movie_languages','Movie_countries','Movie_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2247d48-fe66-4090-beb5-9d775192cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_CMU_movies = pd.read_csv(path+\"/MovieSummaries/movie.metadata.tsv\", sep='\\t', names = NAMES_MOVIES, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2df9b-2033-43ba-994a-5005ae8b0403",
   "metadata": {},
   "source": [
    "## Merging CMU \"Per title\" and IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4b09-c009-4ca0-997f-ea12d0dd9b40",
   "metadata": {},
   "source": [
    "### Using unique index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f507e-b7f0-452e-805d-b1f7d51b22e9",
   "metadata": {},
   "source": [
    "We will try to merge both datsets using a unique index. Few combination of columns have been tried to achieve a satisfying result. Here we show our final choice that where the new index is composed of the name and the release year of the movies. So we supposed that these 2 informations were necessary to distinguish between every movies. As we will see in the following cells it was somethimes not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18788b06-8145-4199-b7f5-689176460028",
   "metadata": {},
   "source": [
    "We keep only the year as release date : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9db008c-74c4-4e04-842c-2bd3a22a83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies[\"Movie_release_date\"]=pd.to_datetime(df_CMU_movies[\"Movie_release_date\"], format='mixed', errors='coerce').dt.year.astype('Int64')\n",
    "# For IMDb\n",
    "IMDb['release_date']=pd.to_datetime(IMDb['release_date'], format='mixed', errors='coerce').dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc599a3-720a-44b6-a36d-a6defc3f3a0e",
   "metadata": {},
   "source": [
    "A new column \"modified name\" is created where we drop the ponctuation and the space. All characters are also in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f900ca60-7369-4a46-ba78-b554e32d717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies_modified_title = df_CMU_movies.Movie_name.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "df_CMU_movies[\"modified_title\"]=df_CMU_movies_modified_title\n",
    "# For IMDb\n",
    "df_IMDb_modified_title = IMDb.Primary_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "IMDb[\"modified_title\"]=df_IMDb_modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be166525-8251-47ce-ab95-3a2cd55dd36f",
   "metadata": {},
   "source": [
    "(copy for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "426816a4-e39a-4c22-ae6d-0a4af590868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_IMDb_copy = IMDb.copy()\n",
    "init_CMU_copy = df_CMU_movies.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea22836-a056-4101-8e1f-95e831de173c",
   "metadata": {},
   "source": [
    "All rows with NAN in Title name / date are dropped :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c1f679-624c-463b-b616-8bba64ad44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903 rows are lost with this operation in CMU dataset.  / 81740 \n",
      "101376 rows are lost with this operation in IMDb dataset. / 696953\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "s1 = df_CMU_movies.shape[0]\n",
    "df_CMU_movies = df_CMU_movies.dropna(subset=['Movie_release_date'])\n",
    "s2 = df_CMU_movies.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset.  / {s1} ')\n",
    "# For IMDb\n",
    "s1 = IMDb.shape[0]\n",
    "IMDb = IMDb.dropna(subset=['release_date'])\n",
    "s2 = IMDb.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset. / {s1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203e8f9-ae42-4e5f-bb7f-94a4b94f4037",
   "metadata": {},
   "source": [
    "Defining new index names based on release date and title : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9e8181-f5e8-4eee-a287-d16826b01953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "new_id_CMU = df_CMU_movies.Movie_release_date.astype(str)+df_CMU_movies.modified_title\n",
    "# For IMDb\n",
    "new_id_IMDb = IMDb.release_date.astype(str)+IMDb.modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5baaf-e746-4b62-ab61-8b1dc5acf471",
   "metadata": {},
   "source": [
    "Once we get the new index, we check that this is a unique index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d7b52a-aba3-4915-b96f-bdde05a1ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU has a unique indexing : False\n",
      "IMDb has a unique indexing : False\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "CMU_movies_newind = df_CMU_movies.copy()\n",
    "CMU_movies_newind.index = new_id_CMU\n",
    "print(f'CMU has a unique indexing : {CMU_movies_newind.index.is_unique}')\n",
    "# For IMDb\n",
    "IMDb_movies_newind = IMDb.copy()\n",
    "IMDb_movies_newind.index = new_id_IMDb\n",
    "print(f'IMDb has a unique indexing : {IMDb_movies_newind.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25090a-7a41-4c89-9ca8-e585d2836159",
   "metadata": {},
   "source": [
    "Unfortunately we have a non unique indexing.<br>Let's count how much rows have the same index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22bb518-88c7-4104-9484-1a669e4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 rows are lost with this operation in CMU dataset  / 74837 \n",
      "CMU has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = CMU_movies_newind.index.duplicated(keep=False)\n",
    "df_CMU_movies_wo_dupl = CMU_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = CMU_movies_newind.shape[0]\n",
    "s2 = df_CMU_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset  / {s1} ')\n",
    "print(f'CMU has a unique indexing : {df_CMU_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8d5cf5c-fbfa-4c50-a5ba-c13677531b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = IMDb_movies_newind.index.duplicated(keep=False)\n",
    "df_IMDb_movies_wo_dupl = IMDb_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = IMDb_movies_newind.shape[0]\n",
    "s2 = df_IMDb_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {df_IMDb_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c738572-94a9-4827-bb36-03306b32124d",
   "metadata": {},
   "source": [
    "As the amount of duplicates are small, we decide to drop all of them for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5f53-12e9-4d44-8ce2-81cfc9bf0364",
   "metadata": {},
   "source": [
    "#### Merge CMU with IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e92e3882-e5c5-4fab-9eba-eca58988d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df_IMDb_movies_wo_dupl.merge(df_CMU_movies_wo_dupl, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf6df670-d15c-4f01-92a4-15e9424e3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 44864 out of 81740 have a match\n"
     ]
    }
   ],
   "source": [
    "print(f'A total of {merged.shape[0]} out of {init_CMU_copy.shape[0]} have a match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab8b55-d6c8-4987-86aa-add60819677b",
   "metadata": {},
   "source": [
    "Previously we used the \"primary title\" of the IMDb dataset to create the \"modified_title\" columns. Let's redo the previous operation on the \"original title\" column of the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1fdda-79f0-45d8-ae1d-9945f711e5cd",
   "metadata": {},
   "source": [
    "We don't start with the entire dataset, but we take the unmatched rows of each dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a47a9017-965a-4cc4-a84f-c6107aa6064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549793, 10)\n",
      "(29698, 10)\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU = df_CMU_movies_wo_dupl[~df_CMU_movies_wo_dupl.index.isin(df_IMDb_movies_wo_dupl.index)]\n",
    "notmatched_IMDb = IMDb_movies_newind[~IMDb_movies_newind.index.isin(df_CMU_movies_wo_dupl.index)]\n",
    "\n",
    "print(notmatched_IMDb.shape)\n",
    "print(notmatched_CMU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793bfce4-4159-42d9-85a2-87de5488ff30",
   "metadata": {},
   "source": [
    "(Same procedure as before but with Original_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ec271f7-25be-46b4-bc0a-4bf8c0b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "notmatched_IMDb = notmatched_IMDb.drop(columns=[\"modified_title\"])\n",
    "df_IMDb_modified_titlev2 = notmatched_IMDb.Original_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "notmatched_IMDb[\"modified_title\"]=df_IMDb_modified_titlev2\n",
    "new_id_IMDbv2 = notmatched_IMDb.release_date.astype(str)+notmatched_IMDb.modified_title\n",
    "\n",
    "notmatched_IMDb_newind = notmatched_IMDb.copy()\n",
    "notmatched_IMDb_newind.index = new_id_IMDbv2\n",
    "\n",
    "mask_duplicate = notmatched_IMDb_newind.index.duplicated(keep=False)\n",
    "notmatched_IMDb_wo_dupl = notmatched_IMDb_newind[~mask_duplicate]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {notmatched_IMDb_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f110e-179b-42dd-be50-d35ba021f2db",
   "metadata": {},
   "source": [
    "We merge the 2 datasets of unmatched rows (that also have unique index) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e5a0143-cd50-44ec-a3b1-aaf8a97bbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We add 3115 rows using the original title !\n"
     ]
    }
   ],
   "source": [
    "mergedv2 = notmatched_IMDb_wo_dupl.merge(notmatched_CMU, left_index=True, right_index=True, how='inner')\n",
    "print(f'We add {mergedv2.shape[0]} rows using the original title !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f2806-97e4-44a1-a184-97640396e7ba",
   "metadata": {},
   "source": [
    "We then concat both merge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ddacbb8-05e2-4671-85d6-c81fe8d8c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge dataset has unique indexing : True\n",
      "Size of dataset : (47979, 20)\n"
     ]
    }
   ],
   "source": [
    "merge_final = pd.concat([merged,mergedv2])\n",
    "print(f'Merge dataset has unique indexing : {merge_final.index.is_unique}')\n",
    "print(f'Size of dataset : {merge_final.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759971de-fe3e-41b8-b31f-b9992a74a77a",
   "metadata": {},
   "source": [
    "As se can see, a non negligeble part of CMU dataset has not find a match : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de5287c9-218e-41b4-a155-38d5aee1207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched rows of CMU dataset : (29698, 10)\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU2 = notmatched_CMU[~notmatched_CMU.index.isin(notmatched_IMDb_wo_dupl.index)]\n",
    "notmatched_IMDb2 = notmatched_IMDb_wo_dupl[~notmatched_IMDb_wo_dupl.index.isin(notmatched_CMU.index)]\n",
    "print(f'Number of unmatched rows of CMU dataset : {notmatched_CMU.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c37f5f-e562-4bab-b755-92494e15c97a",
   "metadata": {},
   "source": [
    "Next section will try a other way to merge these 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542950d-e94a-423d-978d-6d2391f4a174",
   "metadata": {},
   "source": [
    "As the amount of duplicates respectively to their index (release year + name) are almost negligible, we don't take care of adding them in the \"notmatched_CMU2\" and \"notmatched_IMDb2\" for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd57e-37a0-4ed1-9820-e66097e2e5fa",
   "metadata": {},
   "source": [
    "### Merging using Data from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631bfc0-aa48-4832-b03f-afee596dcc07",
   "metadata": {},
   "source": [
    "We try now to get the IMDb id of the movie contained in CMU dataset by scraping wikipedia. This method seems to be really precise to merge both dataset, but it was not consider in the first step due to it's computational time. Now that we've reduced the number of samples, let's see what we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98619833-8dce-48e9-9477-2c6d06e63aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.parse as url\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24d6a2-e93b-4387-9f5d-07e964ccbdca",
   "metadata": {},
   "source": [
    "The function \"get_IMDb_id\" do :\n",
    "- go to wikipedia page of the film using the wikipedia id\n",
    "- search in the HTML text for an URL that starts with \"https://www.imdb.com/title/tt\". (This is the imbd URL of the film)\n",
    "- if there is a single adresse looking like this in the HTML page it return last part of the URL (\"A part of path\") that correspond to the IMDb id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b58e7154-0b4d-4406-9a18-f127f4cdf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMDb_id(wikipedia_id):\n",
    "    # source : \n",
    "    # - https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/\n",
    "    # - https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\n",
    "    r = requests.get(\"https://en.wikipedia.org/?curid=\"+str(wikipedia_id))\n",
    "    \n",
    "    if r.status_code == 404:\n",
    "        return pd.NA\n",
    "        \n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    nbr_link = 0\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile(\"^https://www.imdb.com/title/tt\")}):\n",
    "        nbr_link += 1\n",
    "        href  = link.get('href')\n",
    "        url_parts = url.urlparse(href)\n",
    "        IMBd_id = url_parts.path.split('/')[2]\n",
    "    if nbr_link==1:\n",
    "        return IMBd_id\n",
    "    else :\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45c6ccb-ed6c-47f8-8fd5-77d97fc36775",
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatched_CMU2_copy = notmatched_CMU2.copy()\n",
    "notmatched_CMU2_copy=notmatched_CMU2_copy['Wikipedia_movie_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df7b56d2-81c6-4196-9328-2a43dcb9f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26583/26583 [6:40:28<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "notmatched_CMU2_copy = notmatched_CMU2_copy.progress_apply(lambda x: get_IMDb_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad2326-bd0c-48ed-9db2-8e83d963b72d",
   "metadata": {},
   "source": [
    "Finally we can merge the notmatched dataset and concat it with the merge_dataset that we got using unique indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da9b056-124a-4758-8212-512d3da28b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/w8_bl_xd53n7dlt9q3m5v8vm0000gn/T/ipykernel_27614/483739325.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notmatched_CMU2['IMDb_title_ID'] = notmatched_CMU2_copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9753 new samples have been matched with web scraping !\n",
      "Final score :\n",
      "57732 out of 81740 have been matched\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU2['IMDb_title_ID'] = notmatched_CMU2_copy\n",
    "mergev3 = pd.merge(notmatched_IMDb2, notmatched_CMU2, how = 'inner', on = 'IMDb_title_ID' )\n",
    "print(f'{mergev3.shape[0]} new samples have been matched with web scraping !')\n",
    "final = pd.concat([merge_final,mergev3])\n",
    "print(\"Final score :\")\n",
    "print(f'{final.shape[0]} out of {init_CMU_copy.shape[0]} have been matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c910ad7-886e-4a14-a650-b365c2011c1c",
   "metadata": {},
   "source": [
    "## CMU actor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a75e12-3bed-4701-8801-7de312138e88",
   "metadata": {},
   "source": [
    "Reminder <br>\n",
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provide also a descriptive of the datasets. A more detailed description figures in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6605d-1869-4216-a9ec-88afa069caa6",
   "metadata": {},
   "source": [
    "We are interested now in the character.metadata file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79a431f8-bb1e-4684-8166-3e7a7359c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "character_column = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_release_date','Character_name','actor_DOB','actor_gender','actor_height','actor_ethnicity','actor_name','actor_age_atmovierelease','freebase_character_actor_map_id','freebase_character_id','freebase_actor_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d85c20c8-c654-4ebe-9f35-49fc481deed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "CMU_character = pd.read_csv(path+\"/MovieSummaries/character.metadata.tsv\", sep='\\t',names = character_column, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed32a2c-27a5-4ef2-92ca-e36bb5c1bfb9",
   "metadata": {},
   "source": [
    "We are interested to get a serie that have in index the name of an actor and contain a list of all the film that he plays in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c830a28-cf20-402c-8304-e546e647ba11",
   "metadata": {},
   "source": [
    "We have done it using groupby and apply methods : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e64194a-028b-41d8-a1d7-ca3488671c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_films(df):\n",
    "    list_film = df.Freebase_movie_ID.tolist()\n",
    "    return list_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db70d65f-3c32-4667-8488-92e9716a806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = CMU_character.groupby(by='freebase_actor_id').apply(get_films,include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f75c0-a037-49e8-92b7-0a0eabbb7946",
   "metadata": {},
   "source": [
    "## Save final dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eab40-a163-414a-ac8a-4b9dfb2a3474",
   "metadata": {},
   "source": [
    "All dataset are ready to be used for the analysis. Let's save them in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdd8c830-a162-400c-8f9a-75a9769cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/alexandre/Desktop/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec2f1d5a-778c-4c4e-bed0-f525e7869691",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.to_pickle(path+'/Actor.pkl')\n",
    "final.to_pickle(path+'/Movie.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3f552-d9fc-496d-a87d-183145207ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
