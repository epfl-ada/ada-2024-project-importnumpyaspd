{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c35940-0471-47ca-b1ba-690c0d8f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from T_data_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f8231-996b-44be-a0a5-3edf522c85bb",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212a1c-6acd-4bef-9252-ee1c9b906a96",
   "metadata": {},
   "source": [
    "This notebook aims to explain the preprocessing steps in order to get the datasets that will be used for the analysis, repectively \"Actor.pkl\" and \"Movie.pkl\" <br> All these steps are indicative, you don't have to run all these cells as running all the notebook a quite time-consuming (around 7 hours due to the part \"Data from Web\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8ec3d-fe0a-4b91-90a5-c6b09992892f",
   "metadata": {},
   "source": [
    "## IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec45fe5-7679-4471-9e06-2a1f5767eac1",
   "metadata": {},
   "source": [
    "First, you can go to https://datasets.imdbws.com, download the files and extract them :\n",
    "- title.basics.tsv.gz\n",
    "- title.crew.tsv.gz\n",
    "- title.ratings.tsv.gz\n",
    "- name.basics.tsv.gz\n",
    "  \n",
    "<br> Note that a descriptive of this dataset can be found on https://developer.imdb.com/non-commercial-datasets/ or in the README in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029194e-97d3-4be3-9471-c9f0c062ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the path of the folder you have just downloaded\n",
    "# the folder containing all datasets on your machine :\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5f375-f4ea-452b-85eb-8f7fd5b84a9a",
   "metadata": {},
   "source": [
    "Custom column names are defined :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1163a18-8537-46d3-9031-6e4cf1f1f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_CREW = ['IMDb_title_ID','IMDb_director_ID', 'IMDb_writers_ID']\n",
    "NAMES_ratings = ['IMDb_title_ID', 'Average rating', 'number of votes'] \n",
    "NAMES_BASICS = ['IMDb_people_ID', 'Name', 'birthYear', 'deathYear', 'profession', 'knownForTitles']\n",
    "NAMES_TITLES = ['IMDb_title_ID', 'TitleType', 'Primary_title', 'Original_title', 'isAdult', 'release_date', 'end_year', 'runtime', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a24d5-2398-4cbc-8134-88f5a51697b9",
   "metadata": {},
   "source": [
    "All datasets are loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b82ede-362e-4e45-beeb-10b0790bea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_crew = pd.read_csv(path+\"title.crew.tsv/title.crew.tsv\", sep='\\t', names = NAMES_CREW, header = 0)\n",
    "df_IMDb_ratings = pd.read_csv(path+\"title.ratings.tsv/title.ratings.tsv\", sep='\\t', names = NAMES_ratings, header = 0)\n",
    "df_IMDb_title = pd.read_csv(path+\"title.basics.tsv/title.basics.tsv\", sep='\\t', names = NAMES_TITLES, header = 0, low_memory=False)\n",
    "df_IMDb_name = pd.read_csv(path+\"name.basics.tsv/name.basics.tsv\", sep='\\t', names = NAMES_BASICS, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960f395-8efd-427f-9f37-22c696a23df1",
   "metadata": {},
   "source": [
    "For title dataset, only movie must be selected (drop series,...). Without this set, df_IMDb_title is huge.\n",
    "<br> Few unecessary columns are also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dee27ca-0979-469d-986d-3957453816cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_type = 'movie'\n",
    "df_IMDb_title = df_IMDb_title.query('TitleType==@selected_type')\n",
    "df_IMDb_title=df_IMDb_title[['IMDb_title_ID', 'release_date','runtime','Primary_title', 'Original_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984dfdb-6bf8-40ea-827d-007fefeb8c2f",
   "metadata": {},
   "source": [
    "### Merging Titles, crews and ratings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c0683e-902d-4249-8799-30dea5a7325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696953, 5)\n",
      "(1497560, 3)\n",
      "(10561467, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_IMDb_title.shape)\n",
    "print(df_IMDb_ratings.shape)\n",
    "print(df_IMDb_crew.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c24a49-a482-44ff-b0c2-c3d7719ac922",
   "metadata": {},
   "source": [
    "The ratings and crew dataset contains also ratings of non-movie type. We merge on title datset as we have already selected the movie's row. (here, how = left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bc0958-77a5-4ca4-9fc9-ddaffc003508",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_title_rating = pd.merge(df_IMDb_title, df_IMDb_ratings, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de7186c-3c86-45e0-b8aa-7a0c2800761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb = pd.merge(IMDb_title_rating, df_IMDb_crew, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe1961-f1dd-492f-89b6-012948cf5f66",
   "metadata": {},
   "source": [
    "## CMU \"Per title\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb231a69-7e92-414b-91e0-0a385283066c",
   "metadata": {},
   "source": [
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provide also a descriptive of the datasets. A more detailed description figures in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab0d34-973d-4a64-b9d9-06847e22de11",
   "metadata": {},
   "source": [
    "We are interested in the movie.metadata file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c00601-84b0-4c04-bba2-21085573672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "NAMES_MOVIES = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_name','Movie_release_date','Movie_box_office_revenue','Movie_runtime','Movie_languages','Movie_countries','Movie_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2247d48-fe66-4090-beb5-9d775192cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_CMU_movies = pd.read_csv(path+\"MovieSummaries/movie.metadata.tsv\", sep='\\t', names = NAMES_MOVIES, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2df9b-2033-43ba-994a-5005ae8b0403",
   "metadata": {},
   "source": [
    "## Merging CMU \"Per title\" and IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4b09-c009-4ca0-997f-ea12d0dd9b40",
   "metadata": {},
   "source": [
    "### Using unique index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f507e-b7f0-452e-805d-b1f7d51b22e9",
   "metadata": {},
   "source": [
    "We will try to merge both datsets using a unique index. Few combination of columns have been tried to achieve a satisfying result. Here we show our final choice that where the new index is composed of the name and the release year of the movies. So we supposed that these 2 informations were necessary to distinguish between every movies. As we will see in the following cells it was somethimes not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18788b06-8145-4199-b7f5-689176460028",
   "metadata": {},
   "source": [
    "We keep only the year as release date : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9db008c-74c4-4e04-842c-2bd3a22a83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies[\"Movie_release_date\"]=pd.to_datetime(df_CMU_movies[\"Movie_release_date\"], format='mixed', errors='coerce').dt.year.astype('Int64')\n",
    "# For IMDb\n",
    "IMDb['release_date']=pd.to_datetime(IMDb['release_date'], format='mixed', errors='coerce').dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc599a3-720a-44b6-a36d-a6defc3f3a0e",
   "metadata": {},
   "source": [
    "A new column \"modified name\" is created where we drop the ponctuation and the space. All characters are also in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f900ca60-7369-4a46-ba78-b554e32d717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies_modified_title = df_CMU_movies.Movie_name.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "df_CMU_movies[\"modified_title\"]=df_CMU_movies_modified_title\n",
    "# For IMDb\n",
    "df_IMDb_modified_title = IMDb.Primary_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "IMDb[\"modified_title\"]=df_IMDb_modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be166525-8251-47ce-ab95-3a2cd55dd36f",
   "metadata": {},
   "source": [
    "(copy for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426816a4-e39a-4c22-ae6d-0a4af590868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_IMDb_copy = IMDb.copy()\n",
    "init_CMU_copy = df_CMU_movies.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea22836-a056-4101-8e1f-95e831de173c",
   "metadata": {},
   "source": [
    "All rows with NAN in Title name / date are dropped :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c1f679-624c-463b-b616-8bba64ad44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903 rows are lost with this operation in CMU dataset.  / 81740 \n",
      "101376 rows are lost with this operation in IMDb dataset. / 696953\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "s1 = df_CMU_movies.shape[0]\n",
    "df_CMU_movies = df_CMU_movies.dropna(subset=['Movie_release_date'])\n",
    "s2 = df_CMU_movies.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset.  / {s1} ')\n",
    "# For IMDb\n",
    "s1 = IMDb.shape[0]\n",
    "IMDb = IMDb.dropna(subset=['release_date'])\n",
    "s2 = IMDb.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset. / {s1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772ccdf8-2e79-455b-88fc-86ea0ce7b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of the size of the CMU containing the NAN values : 6903\n"
     ]
    }
   ],
   "source": [
    "# Store a dataset with row with NAN as Title or release date\n",
    "# This step is done in prevision of the section \"Merging using Data from the web\".\n",
    "CMU_with_NAN = init_CMU_copy[~init_CMU_copy.index.isin(df_CMU_movies.index)]\n",
    "print(f'Quick check of the size of the CMU containing the NAN values : {CMU_with_NAN.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203e8f9-ae42-4e5f-bb7f-94a4b94f4037",
   "metadata": {},
   "source": [
    "Defining new index names based on release date and title : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9e8181-f5e8-4eee-a287-d16826b01953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "new_id_CMU = df_CMU_movies.Movie_release_date.astype(str)+df_CMU_movies.modified_title\n",
    "# For IMDb\n",
    "new_id_IMDb = IMDb.release_date.astype(str)+IMDb.modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5baaf-e746-4b62-ab61-8b1dc5acf471",
   "metadata": {},
   "source": [
    "Once we get the new index, we check that this is a unique index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d7b52a-aba3-4915-b96f-bdde05a1ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU has a unique indexing : False\n",
      "IMDb has a unique indexing : False\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "CMU_movies_newind = df_CMU_movies.copy()\n",
    "CMU_movies_newind.index = new_id_CMU\n",
    "print(f'CMU has a unique indexing : {CMU_movies_newind.index.is_unique}')\n",
    "# For IMDb\n",
    "IMDb_movies_newind = IMDb.copy()\n",
    "IMDb_movies_newind.index = new_id_IMDb\n",
    "print(f'IMDb has a unique indexing : {IMDb_movies_newind.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25090a-7a41-4c89-9ca8-e585d2836159",
   "metadata": {},
   "source": [
    "Unfortunately we have a non unique indexing.<br>Let's count how much rows have the same index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d22bb518-88c7-4104-9484-1a669e4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 rows are lost with this operation in CMU dataset  / 74837 \n",
      "CMU has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = CMU_movies_newind.index.duplicated(keep=False)\n",
    "df_CMU_movies_wo_dupl = CMU_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = CMU_movies_newind.shape[0]\n",
    "s2 = df_CMU_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset  / {s1} ')\n",
    "print(f'CMU has a unique indexing : {df_CMU_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8d5cf5c-fbfa-4c50-a5ba-c13677531b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = IMDb_movies_newind.index.duplicated(keep=False)\n",
    "df_IMDb_movies_wo_dupl = IMDb_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = IMDb_movies_newind.shape[0]\n",
    "s2 = df_IMDb_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {df_IMDb_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c738572-94a9-4827-bb36-03306b32124d",
   "metadata": {},
   "source": [
    "As the amount of duplicates are small, we decide to drop all of them for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5f53-12e9-4d44-8ce2-81cfc9bf0364",
   "metadata": {},
   "source": [
    "#### Merge CMU with IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92e3882-e5c5-4fab-9eba-eca58988d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df_IMDb_movies_wo_dupl.merge(df_CMU_movies_wo_dupl, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf6df670-d15c-4f01-92a4-15e9424e3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 44864 out of 81740 have a match\n"
     ]
    }
   ],
   "source": [
    "print(f'A total of {merged.shape[0]} out of {init_CMU_copy.shape[0]} have a match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab8b55-d6c8-4987-86aa-add60819677b",
   "metadata": {},
   "source": [
    "Previously we used the \"primary title\" of the IMDb dataset to create the \"modified_title\" columns. Let's redo the previous operation on the \"original title\" column of the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1fdda-79f0-45d8-ae1d-9945f711e5cd",
   "metadata": {},
   "source": [
    "We don't start with the entire dataset, but we take the unmatched rows of each dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a47a9017-965a-4cc4-a84f-c6107aa6064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549793, 10)\n",
      "(29698, 10)\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU = df_CMU_movies_wo_dupl[~df_CMU_movies_wo_dupl.index.isin(df_IMDb_movies_wo_dupl.index)]\n",
    "notmatched_IMDb = IMDb_movies_newind[~IMDb_movies_newind.index.isin(df_CMU_movies_wo_dupl.index)]\n",
    "\n",
    "print(notmatched_IMDb.shape)\n",
    "print(notmatched_CMU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793bfce4-4159-42d9-85a2-87de5488ff30",
   "metadata": {},
   "source": [
    "(Same procedure as before but with Original_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec271f7-25be-46b4-bc0a-4bf8c0b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "notmatched_IMDb = notmatched_IMDb.drop(columns=[\"modified_title\"])\n",
    "df_IMDb_modified_titlev2 = notmatched_IMDb.Original_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "notmatched_IMDb[\"modified_title\"]=df_IMDb_modified_titlev2\n",
    "new_id_IMDbv2 = notmatched_IMDb.release_date.astype(str)+notmatched_IMDb.modified_title\n",
    "\n",
    "notmatched_IMDb_newind = notmatched_IMDb.copy()\n",
    "notmatched_IMDb_newind.index = new_id_IMDbv2\n",
    "\n",
    "mask_duplicate = notmatched_IMDb_newind.index.duplicated(keep=False)\n",
    "notmatched_IMDb_wo_dupl = notmatched_IMDb_newind[~mask_duplicate]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {notmatched_IMDb_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f110e-179b-42dd-be50-d35ba021f2db",
   "metadata": {},
   "source": [
    "We merge the 2 datasets of unmatched rows (that also have unique index) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e5a0143-cd50-44ec-a3b1-aaf8a97bbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We add 3115 rows using the original title !\n"
     ]
    }
   ],
   "source": [
    "mergedv2 = notmatched_IMDb_wo_dupl.merge(notmatched_CMU, left_index=True, right_index=True, how='inner')\n",
    "print(f'We add {mergedv2.shape[0]} rows using the original title !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f2806-97e4-44a1-a184-97640396e7ba",
   "metadata": {},
   "source": [
    "We then concat both merge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ddacbb8-05e2-4671-85d6-c81fe8d8c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge dataset has unique indexing : True\n",
      "Size of dataset : (47979, 20)\n"
     ]
    }
   ],
   "source": [
    "merge_final = pd.concat([merged,mergedv2])\n",
    "print(f'Merge dataset has unique indexing : {merge_final.index.is_unique}')\n",
    "print(f'Size of dataset : {merge_final.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759971de-fe3e-41b8-b31f-b9992a74a77a",
   "metadata": {},
   "source": [
    "As se can see, a non negligeble part of CMU dataset has not find a match : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de5287c9-218e-41b4-a155-38d5aee1207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26583\n",
      "6903\n",
      "33486\n",
      "Number of unmatched rows of CMU dataset : (33486, 10)\n"
     ]
    }
   ],
   "source": [
    "#CMU\n",
    "notmatched_CMU2 = notmatched_CMU[~notmatched_CMU.index.isin(notmatched_IMDb_wo_dupl.index)]\n",
    "## add also the part of CMU that was dropped since they have a NAN in column \"title or release date\"\n",
    "notmatched_CMU2 = pd.concat([CMU_with_NAN,notmatched_CMU2])\n",
    "#IMDb\n",
    "notmatched_IMDb2 = notmatched_IMDb_wo_dupl[~notmatched_IMDb_wo_dupl.index.isin(notmatched_CMU.index)]\n",
    "\n",
    "print(f'Number of unmatched rows of CMU dataset : {notmatched_CMU2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba570b76-f32e-4261-971a-66d463ede0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Freebase_movie_ID</th>\n",
       "      <th>Movie_name</th>\n",
       "      <th>Movie_release_date</th>\n",
       "      <th>Movie_box_office_revenue</th>\n",
       "      <th>Movie_runtime</th>\n",
       "      <th>Movie_languages</th>\n",
       "      <th>Movie_countries</th>\n",
       "      <th>Movie_genres</th>\n",
       "      <th>modified_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11250635</td>\n",
       "      <td>/m/02r52hc</td>\n",
       "      <td>The Mechanical Monsters</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/03k9fj\": \"...</td>\n",
       "      <td>themechanicalmonsters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>31137877</td>\n",
       "      <td>/m/0gh7n22</td>\n",
       "      <td>Boadicea</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"/m/07s9rl0\": \"Drama\", \"/m/03hn0\": \"Historica...</td>\n",
       "      <td>boadicea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>27374355</td>\n",
       "      <td>/m/0by1_ff</td>\n",
       "      <td>Les Indiens sont encore loin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"/m/0f8l9c\": \"France\", \"/m/06mzp\": \"Switzerla...</td>\n",
       "      <td>{\"/m/07s9rl0\": \"Drama\"}</td>\n",
       "      <td>lesindienssontencoreloin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>29766415</td>\n",
       "      <td>/m/0fp_syp</td>\n",
       "      <td>Donald's Crime</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"/m/02hmvc\": \"Short Film\"}</td>\n",
       "      <td>donaldscrime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>28415406</td>\n",
       "      <td>/m/0crj1f3</td>\n",
       "      <td>The Last Trackers of the Outback</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/0f8l9c\": \"France\", \"/m/0chghy\": \"Australia\"}</td>\n",
       "      <td>{\"/m/0jtdp\": \"Documentary\"}</td>\n",
       "      <td>thelasttrackersoftheoutback</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Wikipedia_movie_ID Freebase_movie_ID                        Movie_name  \\\n",
       "14             11250635        /m/02r52hc           The Mechanical Monsters   \n",
       "73             31137877        /m/0gh7n22                          Boadicea   \n",
       "80             27374355        /m/0by1_ff      Les Indiens sont encore loin   \n",
       "102            29766415        /m/0fp_syp                    Donald's Crime   \n",
       "119            28415406        /m/0crj1f3  The Last Trackers of the Outback   \n",
       "\n",
       "     Movie_release_date  Movie_box_office_revenue  Movie_runtime  \\\n",
       "14                 <NA>                       NaN            NaN   \n",
       "73                 <NA>                       NaN            NaN   \n",
       "80                 <NA>                       NaN           95.0   \n",
       "102                <NA>                       NaN            NaN   \n",
       "119                <NA>                       NaN            NaN   \n",
       "\n",
       "                        Movie_languages  \\\n",
       "14   {\"/m/02h40lc\": \"English Language\"}   \n",
       "73                                   {}   \n",
       "80                                   {}   \n",
       "102                                  {}   \n",
       "119  {\"/m/02h40lc\": \"English Language\"}   \n",
       "\n",
       "                                       Movie_countries  \\\n",
       "14           {\"/m/09c7w0\": \"United States of America\"}   \n",
       "73                                                  {}   \n",
       "80   {\"/m/0f8l9c\": \"France\", \"/m/06mzp\": \"Switzerla...   \n",
       "102                                                 {}   \n",
       "119  {\"/m/0f8l9c\": \"France\", \"/m/0chghy\": \"Australia\"}   \n",
       "\n",
       "                                          Movie_genres  \\\n",
       "14   {\"/m/06n90\": \"Science Fiction\", \"/m/03k9fj\": \"...   \n",
       "73   {\"/m/07s9rl0\": \"Drama\", \"/m/03hn0\": \"Historica...   \n",
       "80                             {\"/m/07s9rl0\": \"Drama\"}   \n",
       "102                        {\"/m/02hmvc\": \"Short Film\"}   \n",
       "119                        {\"/m/0jtdp\": \"Documentary\"}   \n",
       "\n",
       "                  modified_title  \n",
       "14         themechanicalmonsters  \n",
       "73                      boadicea  \n",
       "80      lesindienssontencoreloin  \n",
       "102                 donaldscrime  \n",
       "119  thelasttrackersoftheoutback  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notmatched_CMU2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c37f5f-e562-4bab-b755-92494e15c97a",
   "metadata": {},
   "source": [
    "Next section will try a other way to merge these 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542950d-e94a-423d-978d-6d2391f4a174",
   "metadata": {},
   "source": [
    "As the amount of duplicates respectively to their index (release year + name) are almost negligible, we don't take care of adding them in the \"notmatched_CMU2\" and \"notmatched_IMDb2\" for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd57e-37a0-4ed1-9820-e66097e2e5fa",
   "metadata": {},
   "source": [
    "### Merging using Data from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631bfc0-aa48-4832-b03f-afee596dcc07",
   "metadata": {},
   "source": [
    "We try now to get the IMDb id of the movie contained in CMU dataset by scraping wikipedia. This method seems to be really precise to merge both dataset, but it was not consider in the first step due to it's computational time. Now that we've reduced the number of samples, let's see what we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98619833-8dce-48e9-9477-2c6d06e63aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.parse as url\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24d6a2-e93b-4387-9f5d-07e964ccbdca",
   "metadata": {},
   "source": [
    "The function \"get_IMDb_id\" do :\n",
    "- go to wikipedia page of the film using the wikipedia id\n",
    "- search in the HTML text for an URL that starts with \"https://www.imdb.com/title/tt\". (This is the imbd URL of the film)\n",
    "- if there is a single adresse looking like this in the HTML page it return last part of the URL (\"A part of path\") that correspond to the IMDb id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b58e7154-0b4d-4406-9a18-f127f4cdf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMDb_id(wikipedia_id):\n",
    "    # sources : \n",
    "    # - https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/\n",
    "    # - https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\n",
    "    r = requests.get(\"https://en.wikipedia.org/?curid=\"+str(wikipedia_id))\n",
    "    \n",
    "    if r.status_code == 404:\n",
    "        return pd.NA\n",
    "        \n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    nbr_link = 0\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile(\"^https://www.imdb.com/title/tt\")}):\n",
    "        nbr_link += 1\n",
    "        href  = link.get('href')\n",
    "        url_parts = url.urlparse(href)\n",
    "        IMBd_id = url_parts.path.split('/')[2]\n",
    "    if nbr_link==1:\n",
    "        return IMBd_id\n",
    "    else :\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45c6ccb-ed6c-47f8-8fd5-77d97fc36775",
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatched_CMU2_copy = notmatched_CMU2.copy()\n",
    "notmatched_CMU2_copy=notmatched_CMU2_copy['Wikipedia_movie_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df7b56d2-81c6-4196-9328-2a43dcb9f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26583/26583 [6:40:28<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "notmatched_CMU2_copy = notmatched_CMU2_copy.progress_apply(lambda x: get_IMDb_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad2326-bd0c-48ed-9db2-8e83d963b72d",
   "metadata": {},
   "source": [
    "Finally we can merge the notmatched dataset and concat it with the merge_dataset that we got using unique indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da9b056-124a-4758-8212-512d3da28b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/w8_bl_xd53n7dlt9q3m5v8vm0000gn/T/ipykernel_27614/483739325.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notmatched_CMU2['IMDb_title_ID'] = notmatched_CMU2_copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9753 new samples have been matched with web scraping !\n",
      "Final score :\n",
      "57732 out of 81740 have been matched\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU2['IMDb_title_ID'] = notmatched_CMU2_copy\n",
    "mergev3 = pd.merge(notmatched_IMDb2, notmatched_CMU2, how = 'inner', on = 'IMDb_title_ID' )\n",
    "print(f'{mergev3.shape[0]} new samples have been matched with web scraping !')\n",
    "final = pd.concat([merge_final,mergev3])\n",
    "print(\"Final score :\")\n",
    "print(f'{final.shape[0]} out of {init_CMU_copy.shape[0]} have been matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c910ad7-886e-4a14-a650-b365c2011c1c",
   "metadata": {},
   "source": [
    "## CMU actor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a75e12-3bed-4701-8801-7de312138e88",
   "metadata": {},
   "source": [
    "Reminder <br>\n",
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provide also a descriptive of the datasets. A more detailed description figures in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6605d-1869-4216-a9ec-88afa069caa6",
   "metadata": {},
   "source": [
    "We are interested now in the character.metadata file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79a431f8-bb1e-4684-8166-3e7a7359c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "character_column = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_release_date','Character_name','actor_DOB','actor_gender','actor_height','actor_ethnicity','actor_name','actor_age_atmovierelease','freebase_character_actor_map_id','freebase_character_id','freebase_actor_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d85c20c8-c654-4ebe-9f35-49fc481deed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "CMU_character = pd.read_csv(path+\"/MovieSummaries/character.metadata.tsv\", sep='\\t',names = character_column, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed32a2c-27a5-4ef2-92ca-e36bb5c1bfb9",
   "metadata": {},
   "source": [
    "We are interested to get a serie that have in index the name of an actor and contain a list of all the film that he plays in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c830a28-cf20-402c-8304-e546e647ba11",
   "metadata": {},
   "source": [
    "We have done it using groupby and apply methods : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e64194a-028b-41d8-a1d7-ca3488671c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_films(df):\n",
    "    list_film = df.Freebase_movie_ID.tolist()\n",
    "    return list_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db70d65f-3c32-4667-8488-92e9716a806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = CMU_character.groupby(by='freebase_actor_id').apply(get_films,include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f75c0-a037-49e8-92b7-0a0eabbb7946",
   "metadata": {},
   "source": [
    "## Save final dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eab40-a163-414a-ac8a-4b9dfb2a3474",
   "metadata": {},
   "source": [
    "All dataset are ready to be used for the analysis. Let's save them in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdd8c830-a162-400c-8f9a-75a9769cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/alexandre/Desktop/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec2f1d5a-778c-4c4e-bed0-f525e7869691",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.to_pickle(path+'/Actor.pkl')\n",
    "final.to_pickle(path+'/Movie.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585496c3-2ae4-4286-9b00-f34bef69a282",
   "metadata": {},
   "source": [
    "## Ethnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed3f552-d9fc-496d-a87d-183145207ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thibault Schiesser\\AppData\\Local\\Temp\\ipykernel_2928\\1651754912.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  Characters = pickle.load(file)\n"
     ]
    }
   ],
   "source": [
    "characters_path = 'Character.pkl'\n",
    "with open(characters_path, 'rb') as file:\n",
    "    Characters = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d225c4d0-bdcf-40a9-8408-bb331280b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7edd9c12-16c8-4035-98f4-de0693959678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ethnie(wikipedia_id):\n",
    "\n",
    "    r = requests.get(\"https://www.wikidata.org/wiki/Special:Search\"+wikipedia_id)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    etnies = soup.find('a', href=True, attrs={'data-serp-pos': '0', 'title': True})\n",
    "    if etnies is not None:\n",
    "        abc = etnies.find('span', class_='wb-itemlink-label').text\n",
    "    else :\n",
    "        abc = 0\n",
    "    return abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee74d6f0-bd64-4a3d-a889-923ec5cfd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie_id = pd.unique(Characters['Actor_ethnicity'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a70a722f-3a12-427e-9392-0cdbbeb8a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie = pd.Series(\"ethnie\", index = ethnie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8daa63f6-36a9-4fc0-bc78-d6b7872d4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 93/479 [02:33<10:35,  1.65s/it]\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:Search/m/03w9bjf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002EE20F2FFD0>: Failed to establish a new connection: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002EE20F2FFD0>: Failed to establish a new connection: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:Search/m/03w9bjf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002EE20F2FFD0>: Failed to establish a new connection: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(ethnie_id):\n\u001b[1;32m----> 3\u001b[0m     ethnie\u001b[38;5;241m.\u001b[39miloc[a] \u001b[38;5;241m=\u001b[39m \u001b[43mget_ethnie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[77], line 3\u001b[0m, in \u001b[0;36mget_ethnie\u001b[1;34m(wikipedia_id)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ethnie\u001b[39m(wikipedia_id):\n\u001b[1;32m----> 3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.wikidata.org/wiki/Special:Search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mwikipedia_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(r\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     etnies \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-serp-pos\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /wiki/Special:Search/m/03w9bjf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002EE20F2FFD0>: Failed to establish a new connection: [WinError 10060] Une tentative de connexion a échoué car le parti connecté n’a pas répondu convenablement au-delà d’une certaine durée ou une connexion établie a échoué car l’hôte de connexion n’a pas répondu'))"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in tqdm(ethnie_id):\n",
    "    ethnie.iloc[a] = get_ethnie(i)\n",
    "    a +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23bd8d0d-3a78-4849-a042-9ad1306c84c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/m/044038p                          0\n",
       "/m/0x67               No Lookin' Back\n",
       "/m/064b9n     Omaha Tribe of Nebraska\n",
       "/m/041rx                Jewish people\n",
       "/m/033tf_             Irish Americans\n",
       "/m/04gfy7            Indian Americans\n",
       "/m/0222qb                    Italians\n",
       "/m/01qhm_            German Americans\n",
       "/m/0dryh9k                    Indians\n",
       "/m/048sp5                      Ezhava\n",
       "/m/04mvp8                    Malayali\n",
       "/m/0bzkm2            Taiwanese people\n",
       "/m/02p1pl6                          0\n",
       "/m/0bjbszh                          0\n",
       "/m/022fdt                   Armenians\n",
       "/m/0cqgdq              Marathi people\n",
       "/m/0ffkb4        Lithuanian Americans\n",
       "/m/075dhf0                          0\n",
       "/m/01hwt                 Black people\n",
       "/m/0xnvg            Italian Americans\n",
       "/m/0dqqwy            Danish Americans\n",
       "/m/048z7l               American Jews\n",
       "/m/07bch9          Scottish Americans\n",
       "/m/09v5bdn              Puerto Ricans\n",
       "/m/02w7gg              English people\n",
       "/m/03bkbh                Irish people\n",
       "/m/02vsw1          European Americans\n",
       "/m/09kr66           Russian Americans\n",
       "/m/09vc4s           English Americans\n",
       "/m/0g0x7_             Gujarati people\n",
       "/m/042gtr           Spanish Americans\n",
       "/m/0cm7w1               Bihari people\n",
       "/m/046cwm                        Nair\n",
       "/m/04dbw3             Cuban Americans\n",
       "/m/02ctzb                White people\n",
       "dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnie[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b5525-65e3-4bb7-8bc7-cb3153cf0002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
