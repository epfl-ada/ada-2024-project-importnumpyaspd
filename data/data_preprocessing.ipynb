{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c35940-0471-47ca-b1ba-690c0d8f5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse as url\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f8231-996b-44be-a0a5-3edf522c85bb",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212a1c-6acd-4bef-9252-ee1c9b906a96",
   "metadata": {},
   "source": [
    "This notebook aims to explain the preprocessing steps in order to get the datasets that will be used for the analysis, repectively \"Actor.pkl\" and \"Movie.pkl\" <br> All these steps are indicative, you don't have to run all these cells as running all the notebook a quite time-consuming (around 7 hours due to the part \"Data from Web\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8ec3d-fe0a-4b91-90a5-c6b09992892f",
   "metadata": {},
   "source": [
    "## IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec45fe5-7679-4471-9e06-2a1f5767eac1",
   "metadata": {},
   "source": [
    "First, you can go to https://datasets.imdbws.com, download the files and extract them :\n",
    "- title.basics.tsv.gz\n",
    "- title.crew.tsv.gz\n",
    "- title.ratings.tsv.gz\n",
    "- name.basics.tsv.gz\n",
    "  \n",
    "<br> Note that a descriptive of this dataset can be found on https://developer.imdb.com/non-commercial-datasets/ or in the README in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029194e-97d3-4be3-9471-c9f0c062ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the path of the folder you have just downloaded\n",
    "# the folder containing all datasets on your machine :\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5f375-f4ea-452b-85eb-8f7fd5b84a9a",
   "metadata": {},
   "source": [
    "Custom column names are defined :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1163a18-8537-46d3-9031-6e4cf1f1f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_CREW = ['IMDb_title_ID','IMDb_director_ID', 'IMDb_writers_ID']\n",
    "NAMES_ratings = ['IMDb_title_ID', 'Average rating', 'number of votes'] \n",
    "NAMES_BASICS = ['IMDb_people_ID', 'Name', 'birthYear', 'deathYear', 'profession', 'knownForTitles']\n",
    "NAMES_TITLES = ['IMDb_title_ID', 'TitleType', 'Primary_title', 'Original_title', 'isAdult', 'release_date', 'end_year', 'runtime', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a24d5-2398-4cbc-8134-88f5a51697b9",
   "metadata": {},
   "source": [
    "All datasets are loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b82ede-362e-4e45-beeb-10b0790bea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_crew = pd.read_csv(path+\"title.crew.tsv/title.crew.tsv\", sep='\\t', names = NAMES_CREW, header = 0)\n",
    "df_IMDb_ratings = pd.read_csv(path+\"title.ratings.tsv/title.ratings.tsv\", sep='\\t', names = NAMES_ratings, header = 0)\n",
    "df_IMDb_title = pd.read_csv(path+\"title.basics.tsv/title.basics.tsv\", sep='\\t', names = NAMES_TITLES, header = 0, low_memory=False)\n",
    "df_IMDb_name = pd.read_csv(path+\"name.basics.tsv/name.basics.tsv\", sep='\\t', names = NAMES_BASICS, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960f395-8efd-427f-9f37-22c696a23df1",
   "metadata": {},
   "source": [
    "For title dataset, only movie must be selected (drop series,...). Without this set, df_IMDb_title is huge.\n",
    "<br> Few unecessary columns are also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dee27ca-0979-469d-986d-3957453816cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_type = 'movie'\n",
    "df_IMDb_title = df_IMDb_title.query('TitleType==@selected_type')\n",
    "df_IMDb_title=df_IMDb_title[['IMDb_title_ID', 'release_date','runtime','Primary_title', 'Original_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4984dfdb-6bf8-40ea-827d-007fefeb8c2f",
   "metadata": {},
   "source": [
    "### Merging Titles, crews and ratings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c0683e-902d-4249-8799-30dea5a7325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(696953, 5)\n",
      "(1497560, 3)\n",
      "(10561467, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_IMDb_title.shape)\n",
    "print(df_IMDb_ratings.shape)\n",
    "print(df_IMDb_crew.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c24a49-a482-44ff-b0c2-c3d7719ac922",
   "metadata": {},
   "source": [
    "The ratings and crew dataset contains also ratings of non-movie type. We merge on title datset as we have already selected the movie's row. (here, how = left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bc0958-77a5-4ca4-9fc9-ddaffc003508",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_title_rating = pd.merge(df_IMDb_title, df_IMDb_ratings, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de7186c-3c86-45e0-b8aa-7a0c2800761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb = pd.merge(IMDb_title_rating, df_IMDb_crew, how='left', on = 'IMDb_title_ID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c777517-3859-43a9-9819-c78b1d39de76",
   "metadata": {},
   "source": [
    "IMDb dataset contains sometimes the value \"/N\" when an information is unknown. Let's replace them by an NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2425fa6-95f0-4cbd-af37-df8536f92933",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb.replace(r'\\N', pd.NA, inplace=True)\n",
    "df_IMDb_name.replace(r'\\N', pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f57636-e184-408b-81d6-2ba3d1cc39ae",
   "metadata": {},
   "source": [
    "### Match id of writer/director with their name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd778f47-2cce-4776-aefc-877dcd07b46b",
   "metadata": {},
   "source": [
    "IMDb dataset contains only the id of the writer and producter but not their name. In this section we add 2 columns with the name of each one (if known)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d7669-1cea-46dd-a162-c0f4006be183",
   "metadata": {},
   "source": [
    "Defining a dict to be able to find the match easily :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f740f859-593b-4bce-b9f4-667eacf5d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IMDb_name.index = df_IMDb_name[\"IMDb_people_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacd328e-188c-4aff-adee-4d49d4df3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name = df_IMDb_name[[\"Name\",\"birthYear\",\"deathYear\"]].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae7aad-4323-49d8-985d-e2cebb813b66",
   "metadata": {},
   "source": [
    "Add a column for the name, birth year and death year (for each writer and producter). if there are multiple writer/producter, the infos are separated by comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0076f3c3-8a19-4199-832e-73cd8061cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['Producer name'] = IMDb['IMDb_director_ID'].map(lambda x: \", \".join(dict_name['Name'].get(i, 'NAN') for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abbd9a0-bc85-4336-aba2-3f43f21d0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['birthYear producer'] = IMDb['IMDb_director_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['birthYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8029a1c8-b400-4071-a2dd-9520ac083a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['deathYear producer'] = IMDb['IMDb_director_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['deathYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd17337c-4c00-4c6e-aa7e-9e27efdd9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['Writer name'] = IMDb['IMDb_writers_ID'].map(lambda x: \", \".join(dict_name['Name'].get(i, 'NAN') for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3020adbd-a874-483a-bd9a-d9bc79776395",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['birthYear writer'] = IMDb['IMDb_writers_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['birthYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb889c5-e565-4763-8034-630cebabf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb['deathYear writer'] = IMDb['IMDb_writers_ID'].map(\n",
    "    lambda x: \", \".join(str(dict_name['deathYear'].get(i, 'NAN')) for i in x.split(',')) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe1961-f1dd-492f-89b6-012948cf5f66",
   "metadata": {},
   "source": [
    "## CMU \"Per title\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb231a69-7e92-414b-91e0-0a385283066c",
   "metadata": {},
   "source": [
    "You can go to https://www.cs.cmu.edu/~ark/personas/, download the dataset and extract all files. This webpage provide also a descriptive of the datasets. A more detailed description figures in the README of the download folder or in https://github.com/epfl-ada/ada-2024-project-importnumpyaspd/tree/main/data/CMU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab0d34-973d-4a64-b9d9-06847e22de11",
   "metadata": {},
   "source": [
    "We are interested in the movie.metadata file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c00601-84b0-4c04-bba2-21085573672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom column names :\n",
    "NAMES_MOVIES = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_name','Movie_release_date','Movie_box_office_revenue','Movie_runtime','Movie_languages','Movie_countries','Movie_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2247d48-fe66-4090-beb5-9d775192cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df_CMU_movies = pd.read_csv(path+\"MovieSummaries/movie.metadata.tsv\", sep='\\t', names = NAMES_MOVIES, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f44b7-8fcb-444c-b46f-5a0a8a6de8c9",
   "metadata": {},
   "source": [
    "Columns \"Movie_genres\", \"Movie_countries\", \"Movie_languages\" have a dictionnary format containing the id. Let's keep only the values and join each element by a \",\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c7a7beb-255d-4bcb-b31f-93b3f5cecb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CMU_movies[\"Movie_languages\"] = df_CMU_movies[\"Movie_languages\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)\n",
    "df_CMU_movies[\"Movie_countries\"] = df_CMU_movies[\"Movie_countries\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)\n",
    "df_CMU_movies[\"Movie_genres\"] = df_CMU_movies[\"Movie_genres\"].apply(lambda x: \", \".join(ast.literal_eval(x).values()) if pd.notna(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2df9b-2033-43ba-994a-5005ae8b0403",
   "metadata": {},
   "source": [
    "## Merging CMU \"Per title\" and IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4b09-c009-4ca0-997f-ea12d0dd9b40",
   "metadata": {},
   "source": [
    "### Using unique index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f507e-b7f0-452e-805d-b1f7d51b22e9",
   "metadata": {},
   "source": [
    "We will try to merge both datsets using a unique index. Few combination of columns have been tried to achieve a satisfying result. Here we show our final choice that where the new index is composed of the name and the release year of the movies. So we supposed that these 2 informations were necessary to distinguish between every movies. As we will see in the following cells it was somethimes not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18788b06-8145-4199-b7f5-689176460028",
   "metadata": {},
   "source": [
    "We keep only the year as release date : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9db008c-74c4-4e04-842c-2bd3a22a83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies[\"Movie_release_date\"]=pd.to_datetime(df_CMU_movies[\"Movie_release_date\"], format='mixed', errors='coerce').dt.year.astype('Int64')\n",
    "# For IMDb\n",
    "IMDb['release_date']=pd.to_datetime(IMDb['release_date'], format='mixed', errors='coerce').dt.year.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc599a3-720a-44b6-a36d-a6defc3f3a0e",
   "metadata": {},
   "source": [
    "A new column \"modified name\" is created where we drop the ponctuation and the space. All characters are also in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f900ca60-7369-4a46-ba78-b554e32d717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "df_CMU_movies_modified_title = df_CMU_movies.Movie_name.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "df_CMU_movies[\"modified_title\"]=df_CMU_movies_modified_title\n",
    "# For IMDb\n",
    "df_IMDb_modified_title = IMDb.Primary_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "IMDb[\"modified_title\"]=df_IMDb_modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be166525-8251-47ce-ab95-3a2cd55dd36f",
   "metadata": {},
   "source": [
    "(copy for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426816a4-e39a-4c22-ae6d-0a4af590868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_IMDb_copy = IMDb.copy()\n",
    "init_CMU_copy = df_CMU_movies.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea22836-a056-4101-8e1f-95e831de173c",
   "metadata": {},
   "source": [
    "All rows with NAN in Title name / date are dropped :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45c1f679-624c-463b-b616-8bba64ad44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903 rows are lost with this operation in CMU dataset.  / 81740 \n",
      "101376 rows are lost with this operation in IMDb dataset. / 696953\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "s1 = df_CMU_movies.shape[0]\n",
    "df_CMU_movies = df_CMU_movies.dropna(subset=['Movie_release_date'])\n",
    "s2 = df_CMU_movies.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset.  / {s1} ')\n",
    "# For IMDb\n",
    "s1 = IMDb.shape[0]\n",
    "IMDb = IMDb.dropna(subset=['release_date'])\n",
    "s2 = IMDb.shape[0]\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset. / {s1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772ccdf8-2e79-455b-88fc-86ea0ce7b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick check of the size of the CMU containing the NAN values : 6903\n"
     ]
    }
   ],
   "source": [
    "# Store a dataset with row with NAN as Title or release date\n",
    "# This step is done in prevision of the section \"Merging using Data from the web\".\n",
    "CMU_with_NAN = init_CMU_copy[~init_CMU_copy.index.isin(df_CMU_movies.index)]\n",
    "print(f'Quick check of the size of the CMU containing the NAN values : {CMU_with_NAN.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203e8f9-ae42-4e5f-bb7f-94a4b94f4037",
   "metadata": {},
   "source": [
    "Defining new index names based on release date and title : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9e8181-f5e8-4eee-a287-d16826b01953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMU\n",
    "new_id_CMU = df_CMU_movies.Movie_release_date.astype(str)+df_CMU_movies.modified_title\n",
    "# For IMDb\n",
    "new_id_IMDb = IMDb.release_date.astype(str)+IMDb.modified_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5baaf-e746-4b62-ab61-8b1dc5acf471",
   "metadata": {},
   "source": [
    "Once we get the new index, we check that this is a unique index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9d7b52a-aba3-4915-b96f-bdde05a1ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU has a unique indexing : False\n",
      "IMDb has a unique indexing : False\n"
     ]
    }
   ],
   "source": [
    "# For CMU\n",
    "CMU_movies_newind = df_CMU_movies.copy()\n",
    "CMU_movies_newind.index = new_id_CMU\n",
    "print(f'CMU has a unique indexing : {CMU_movies_newind.index.is_unique}')\n",
    "# For IMDb\n",
    "IMDb_movies_newind = IMDb.copy()\n",
    "IMDb_movies_newind.index = new_id_IMDb\n",
    "print(f'IMDb has a unique indexing : {IMDb_movies_newind.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25090a-7a41-4c89-9ca8-e585d2836159",
   "metadata": {},
   "source": [
    "Unfortunately we have a non unique indexing.<br>Let's count how much rows have the same index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d22bb518-88c7-4104-9484-1a669e4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 rows are lost with this operation in CMU dataset  / 74837 \n",
      "CMU has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = CMU_movies_newind.index.duplicated(keep=False)\n",
    "df_CMU_movies_wo_dupl = CMU_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = CMU_movies_newind.shape[0]\n",
    "s2 = df_CMU_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in CMU dataset  / {s1} ')\n",
    "print(f'CMU has a unique indexing : {df_CMU_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8d5cf5c-fbfa-4c50-a5ba-c13677531b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "mask_duplicate = IMDb_movies_newind.index.duplicated(keep=False)\n",
    "df_IMDb_movies_wo_dupl = IMDb_movies_newind[~mask_duplicate]\n",
    "\n",
    "s1 = IMDb_movies_newind.shape[0]\n",
    "s2 = df_IMDb_movies_wo_dupl.shape[0]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {df_IMDb_movies_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c738572-94a9-4827-bb36-03306b32124d",
   "metadata": {},
   "source": [
    "As the amount of duplicates are small, we decide to drop all of them for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5f53-12e9-4d44-8ce2-81cfc9bf0364",
   "metadata": {},
   "source": [
    "#### Merge CMU with IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e92e3882-e5c5-4fab-9eba-eca58988d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df_IMDb_movies_wo_dupl.merge(df_CMU_movies_wo_dupl, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf6df670-d15c-4f01-92a4-15e9424e3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 44864 out of 81740 have a match\n"
     ]
    }
   ],
   "source": [
    "print(f'A total of {merged.shape[0]} out of {init_CMU_copy.shape[0]} have a match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab8b55-d6c8-4987-86aa-add60819677b",
   "metadata": {},
   "source": [
    "Previously we used the \"primary title\" of the IMDb dataset to create the \"modified_title\" columns. Let's redo the previous operation on the \"original title\" column of the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1fdda-79f0-45d8-ae1d-9945f711e5cd",
   "metadata": {},
   "source": [
    "We don't start with the entire dataset, but we take the unmatched rows of each dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a47a9017-965a-4cc4-a84f-c6107aa6064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549793, 16)\n",
      "(29698, 10)\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU = df_CMU_movies_wo_dupl[~df_CMU_movies_wo_dupl.index.isin(df_IMDb_movies_wo_dupl.index)]\n",
    "notmatched_IMDb = IMDb_movies_newind[~IMDb_movies_newind.index.isin(df_CMU_movies_wo_dupl.index)]\n",
    "\n",
    "print(notmatched_IMDb.shape)\n",
    "print(notmatched_CMU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793bfce4-4159-42d9-85a2-87de5488ff30",
   "metadata": {},
   "source": [
    "(Same procedure as before but with Original_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ec271f7-25be-46b4-bc0a-4bf8c0b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11132 rows are lost with this operation in IMDb dataset / 595577\n",
      "IMDb has a unique indexing : True\n"
     ]
    }
   ],
   "source": [
    "notmatched_IMDb = notmatched_IMDb.drop(columns=[\"modified_title\"])\n",
    "df_IMDb_modified_titlev2 = notmatched_IMDb.Original_title.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\s+', '', regex=True).str.lower()\n",
    "notmatched_IMDb[\"modified_title\"]=df_IMDb_modified_titlev2\n",
    "new_id_IMDbv2 = notmatched_IMDb.release_date.astype(str)+notmatched_IMDb.modified_title\n",
    "\n",
    "notmatched_IMDb_newind = notmatched_IMDb.copy()\n",
    "notmatched_IMDb_newind.index = new_id_IMDbv2\n",
    "\n",
    "mask_duplicate = notmatched_IMDb_newind.index.duplicated(keep=False)\n",
    "notmatched_IMDb_wo_dupl = notmatched_IMDb_newind[~mask_duplicate]\n",
    "\n",
    "print(f'{s1-s2} rows are lost with this operation in IMDb dataset / {s1}')\n",
    "print(f'IMDb has a unique indexing : {notmatched_IMDb_wo_dupl.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f110e-179b-42dd-be50-d35ba021f2db",
   "metadata": {},
   "source": [
    "We merge the 2 datasets of unmatched rows (that also have unique index) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e5a0143-cd50-44ec-a3b1-aaf8a97bbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We add 3115 rows using the original title !\n"
     ]
    }
   ],
   "source": [
    "mergedv2 = notmatched_IMDb_wo_dupl.merge(notmatched_CMU, left_index=True, right_index=True, how='inner')\n",
    "print(f'We add {mergedv2.shape[0]} rows using the original title !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f2806-97e4-44a1-a184-97640396e7ba",
   "metadata": {},
   "source": [
    "We then concat both merge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ddacbb8-05e2-4671-85d6-c81fe8d8c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge dataset has unique indexing : True\n",
      "Size of dataset : (47979, 26)\n"
     ]
    }
   ],
   "source": [
    "merge_final = pd.concat([merged,mergedv2])\n",
    "print(f'Merge dataset has unique indexing : {merge_final.index.is_unique}')\n",
    "print(f'Size of dataset : {merge_final.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759971de-fe3e-41b8-b31f-b9992a74a77a",
   "metadata": {},
   "source": [
    "As se can see, a non negligeble part of CMU dataset has not find a match : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de5287c9-218e-41b4-a155-38d5aee1207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched rows of CMU dataset : (33486, 10)\n"
     ]
    }
   ],
   "source": [
    "#CMU\n",
    "notmatched_CMU2 = notmatched_CMU[~notmatched_CMU.index.isin(notmatched_IMDb_wo_dupl.index)]\n",
    "## add also the part of CMU that was dropped since they have a NAN in column \"title or release date\"\n",
    "notmatched_CMU2 = pd.concat([CMU_with_NAN,notmatched_CMU2])\n",
    "#IMDb\n",
    "notmatched_IMDb2 = notmatched_IMDb_wo_dupl[~notmatched_IMDb_wo_dupl.index.isin(notmatched_CMU.index)]\n",
    "\n",
    "print(f'Number of unmatched rows of CMU dataset : {notmatched_CMU2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c37f5f-e562-4bab-b755-92494e15c97a",
   "metadata": {},
   "source": [
    "Next section will try a other way to merge these 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542950d-e94a-423d-978d-6d2391f4a174",
   "metadata": {},
   "source": [
    "As the amount of duplicates respectively to their index (release year + name) are almost negligible, we don't take care of adding them in the \"notmatched_CMU2\" and \"notmatched_IMDb2\" for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd57e-37a0-4ed1-9820-e66097e2e5fa",
   "metadata": {},
   "source": [
    "### Merging using Data from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631bfc0-aa48-4832-b03f-afee596dcc07",
   "metadata": {},
   "source": [
    "We try now to get the IMDb id of the movie contained in CMU dataset by scraping wikipedia. This method seems to be really precise to merge both dataset, but it was not consider in the first step due to it's computational time. Now that we've reduced the number of samples, let's see what we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24d6a2-e93b-4387-9f5d-07e964ccbdca",
   "metadata": {},
   "source": [
    "The function \"get_IMDb_id\" do :\n",
    "- go to wikipedia page of the film using the wikipedia id\n",
    "- search in the HTML text for an URL that starts with \"https://www.imdb.com/title/tt\". (This is the imbd URL of the film)\n",
    "- if there is a single adresse looking like this in the HTML page it return last part of the URL (\"A part of path\") that correspond to the IMDb id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b58e7154-0b4d-4406-9a18-f127f4cdf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IMDb_id(wikipedia_id):\n",
    "    # sources : \n",
    "    # - https://www.geeksforgeeks.org/beautifulsoup-scraping-link-from-html/\n",
    "    # - https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\n",
    "    r = requests.get(\"https://en.wikipedia.org/?curid=\"+str(wikipedia_id))\n",
    "    \n",
    "    if r.status_code == 404:\n",
    "        return pd.NA\n",
    "        \n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    nbr_link = 0\n",
    "    for link in soup.find_all('a',attrs={'href': re.compile(\"^https://www.imdb.com/title/tt\")}):\n",
    "        nbr_link += 1\n",
    "        href  = link.get('href')\n",
    "        url_parts = url.urlparse(href)\n",
    "        IMBd_id = url_parts.path.split('/')[2]\n",
    "    if nbr_link==1:\n",
    "        return IMBd_id\n",
    "    else :\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e45c6ccb-ed6c-47f8-8fd5-77d97fc36775",
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatched_CMU2_copy = notmatched_CMU2.copy()\n",
    "notmatched_CMU2_copy=notmatched_CMU2_copy['Wikipedia_movie_ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df7b56d2-81c6-4196-9328-2a43dcb9f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "notmatched_CMU2_copy = notmatched_CMU2_copy.progress_apply(lambda x: get_IMDb_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad2326-bd0c-48ed-9db2-8e83d963b72d",
   "metadata": {},
   "source": [
    "Finally we can merge the notmatched dataset and concat it with the merge_dataset that we got using unique indexing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da9b056-124a-4758-8212-512d3da28b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 new samples have been matched with web scraping !\n",
      "Final score :\n",
      "47983 out of 81740 have been matched\n"
     ]
    }
   ],
   "source": [
    "notmatched_CMU2.loc[:,'IMDb_title_ID'] = notmatched_CMU2_copy\n",
    "# à enlever si le tout fonctionne : notmatched_CMU2['IMDb_title_ID'] = notmatched_CMU2_copy\n",
    "mergev3 = pd.merge(notmatched_IMDb2, notmatched_CMU2, how = 'inner', on = 'IMDb_title_ID' )\n",
    "print(f'{mergev3.shape[0]} new samples have been matched with web scraping !')\n",
    "final = pd.concat([merge_final,mergev3])\n",
    "print(\"Final score :\")\n",
    "print(f'{final.shape[0]} out of {init_CMU_copy.shape[0]} have been matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cd0a2ed-f3fc-4466-9fd3-4785423dd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.replace(np.nan, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585496c3-2ae4-4286-9b00-f34bef69a282",
   "metadata": {},
   "source": [
    "## Match id with name / ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125c867-c747-40d7-a061-8e8e78fdd08c",
   "metadata": {},
   "source": [
    "### Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0711932-b905-43ac-b7ad-014bc723f8a8",
   "metadata": {},
   "source": [
    "Loading character dataset : <br> (it's a pickle file but it exactly the raw dataset \"character.metadata.tsv\" (in a different format) downloaded with the other CMU dataset as explained previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32bf0677-5d04-4e11-ae6e-7d607059a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.read_pickle('Character.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4f8c4-95b2-4ab1-8420-59b2bb859b9b",
   "metadata": {},
   "source": [
    "CMU character dataset contains only the freebase id of the ethnicity. Then, we have to match the id with the ethnicity.<br> In order to do it we will scrape data from the web. Here are the steps that we follow :\n",
    "- make a research in wikidata for an ethnicity id\n",
    "- get the HTML text of the research\n",
    "- look at the first URL associated with this research (we suppose that the object of the research (the id) is sufficently \"precise\" to have only one URL)\n",
    "- finally we get the label of the corresponding itemlink (which correspond to the class_ = wb-itemlink-label in wikidata)\n",
    "\n",
    "<br> These steps are done using function \"get_ethnie()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7edd9c12-16c8-4035-98f4-de0693959678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ethnie(ethnie_id):\n",
    "\n",
    "    r = requests.get(\"https://www.wikidata.org/wiki/Special:Search\"+ethnie_id)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    # interested in the first link : \"data-serp-pos :0 \".\n",
    "    etnies = soup.find('a', href=True, attrs={'data-serp-pos': '0', 'title': True})\n",
    "    if etnies is not None:\n",
    "        # get the label of the link : \"wb-itemlink-label\"\n",
    "        abc = etnies.find('span', class_='wb-itemlink-label').text\n",
    "    else :\n",
    "        # return 0 if nothing is found in wikidata\n",
    "        abc = 0\n",
    "    return abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58549-69ab-4d8b-84be-a7815286f222",
   "metadata": {},
   "source": [
    "Creating a numpy array that contains all unique id of the ethnicities :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee74d6f0-bd64-4a3d-a889-923ec5cfd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie_id = pd.unique(characters['Actor_ethnicity'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1b9fa-54ce-407e-aae3-5eecca0992e1",
   "metadata": {},
   "source": [
    "Creating a series to store the ethnicity names :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a70a722f-3a12-427e-9392-0cdbbeb8a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie = pd.Series(\"ethnie\", index = ethnie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2b496b5-a3a1-4f93-a31a-834fa99bb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie.name = \"ethnicity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b6430-9935-46b3-9498-20fc8db317ab",
   "metadata": {},
   "source": [
    "Scrape data from web as describe before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8daa63f6-36a9-4fc0-bc78-d6b7872d4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 106/479 [03:32<12:29,  2.01s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(ethnie_id):\n\u001b[1;32m----> 3\u001b[0m     ethnie\u001b[38;5;241m.\u001b[39miloc[a] \u001b[38;5;241m=\u001b[39m \u001b[43mget_ethnie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00methnie[ethnie\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ethnicities id have not found a match.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m, in \u001b[0;36mget_ethnie\u001b[1;34m(ethnie_id)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ethnie\u001b[39m(ethnie_id):\n\u001b[1;32m----> 3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.wikidata.org/wiki/Special:Search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43methnie_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(r\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# interested in the first link : \"data-serp-pos :0 \".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    266\u001b[0m         req,\n\u001b[0;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Applied_data_analysis\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in tqdm(ethnie_id):\n",
    "    ethnie.iloc[a] = get_ethnie(i)\n",
    "    a +=1\n",
    "print(f'A total of {ethnie[ethnie == 0].count()} ethnicities id have not found a match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca285288-32c3-4fa7-8f8d-e29d2d7c97f6",
   "metadata": {},
   "source": [
    "As this method is not fully reliable, we check visually all the ethnicities and change their value after a web research of the freebase key :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "479b30d0-76f2-419a-ae2c-813f8d170132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie[\"/m/0x67\"] = \"African Americans\"\n",
    "ethnie[\"/m/01trsl\"] = \"First Nations\"\n",
    "ethnie[\"/m/05748\"] = \"Maori\"\n",
    "ethnie[\"/m/059_w\"] = \"Native Americans\"\n",
    "ethnie[\"/m/04zjjt\"] = 0\n",
    "ethnie[\"/m/059v8\"] = \"Nez Perce\"\n",
    "ethnie[\"/m/01d7kx\"] = \"Swedish-speaking population of Finland\"\n",
    "ethnie[\"/m/04_hr_\"] = \"Chinese Singaporeans\"\n",
    "ethnie[\"/m/064pj\"] = \"Persians\"\n",
    "ethnie[\"/m/05ms3p0\"] = \"Punjabi diaspora\"\n",
    "ethnie[\"/m/09snp5\"] = \"Muhajir\"\n",
    "ethnie[\"/m/09cd0m\"] = \"Filipino mestizo\"\n",
    "ethnie[\"/m/03x1x\"] = \"Iroquois\"\n",
    "ethnie[\"/m/012fh\"] = \"Afrikaners\"\n",
    "ethnie[\"/m/05vhv7\"] = 0\n",
    "ethnie[\"/m/04c28\"] = \"Kurds\"\n",
    "ethnie[\"/m/03sk2\"] = 0\n",
    "ethnie[\"/m/0xff\"] = \"Arabs\"\n",
    "ethnie[\"/m/03cdk7b\"] = \"British Pakistanis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd41227-6b1a-4f01-8c1a-547b34f7e73e",
   "metadata": {},
   "source": [
    "The \"0\" values correspond that the id is not known is Wikidata. We replace all 0 by NA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d703608-528e-49e6-8061-2aa7cffd66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnie.replace(0, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b327b-53de-46d6-9b2b-9b4d25f14ac9",
   "metadata": {},
   "source": [
    "Now we can merge character dataset with the \"complete ethnie\" dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55dbf859-f908-4ce6-9886-8ee301988e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.merge(characters, ethnie, left_on = 'Actor_ethnicity', right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "301f998d-a8f9-4a7f-89d0-ee0e5b2c4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.replace(np.nan, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd1f40e6-7afb-4fa5-8251-2c88029a0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actors = (\n",
    "    characters.groupby(\"Freebase_actor_ID\")\n",
    "    .agg({\n",
    "        \"Actor_Name\": \"first\",  \n",
    "        \"Actor_DOB\": \"first\",  \n",
    "        \"Actor_gender\": \"first\",  \n",
    "        \"Actor_height\": \"mean\",\n",
    "        \"Actor_ethnicity\": \"first\",\n",
    "        \"Freebase_movie_ID\": list, \n",
    "        \"Actor_age_at_movie_release\": list, \n",
    "    })\n",
    "    .reset_index() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f75c0-a037-49e8-92b7-0a0eabbb7946",
   "metadata": {},
   "source": [
    "## Save final dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363eab40-a163-414a-ac8a-4b9dfb2a3474",
   "metadata": {},
   "source": [
    "All dataset are ready to be used for the analysis. Let's save them in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdd8c830-a162-400c-8f9a-75a9769cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/Users/alexandre/Desktop/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec2f1d5a-778c-4c4e-bed0-f525e7869691",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_pickle(path+'/Movie.pkl')\n",
    "Actors.to_pickle(path+'actor.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
